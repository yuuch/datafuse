{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"datatypes/float/","text":"Float32 - float Float64 - double","title":"Float Types"},{"location":"datatypes/integer/","text":"Int Ranges \u00b6 Int8 - [-128 : 127] Int16 - [-32768 : 32767] Int32 - [-2147483648 : 2147483647] Int64 - [-9223372036854775808 : 9223372036854775807] UInt Ranges \u00b6 UInt8 - [0 : 255] UInt16 - [0 : 65535] UInt32 - [0 : 4294967295] UInt64 - [0 : 18446744073709551615]","title":"Integer Types"},{"location":"datatypes/integer/#int-ranges","text":"Int8 - [-128 : 127] Int16 - [-32768 : 32767] Int32 - [-2147483648 : 2147483647] Int64 - [-9223372036854775808 : 9223372036854775807]","title":"Int Ranges"},{"location":"datatypes/integer/#uint-ranges","text":"UInt8 - [0 : 255] UInt16 - [0 : 65535] UInt32 - [0 : 4294967295] UInt64 - [0 : 18446744073709551615]","title":"UInt Ranges"},{"location":"datatypes/string/","text":"Strings of an arbitrary length. The length is not limited. The value can contain an arbitrary set of bytes, including null bytes. The String type replaces the types VARCHAR, BLOB, CLOB, and others from other DBMSs.","title":"String Types"},{"location":"development/coding-guidelines/","text":"This document describes the coding guidelines for the Datafuse Rust codebase. Code formatting \u00b6 All code formatting is enforced with rustfmt with a project-specific configuration. Below is an example command: $ make fmt Code analysis \u00b6 Clippy is used to catch common mistakes and is run as a part of continuous integration. Before submitting your code for review, you can run lint: $ make lint Code documentation \u00b6 Any public fields, functions, and methods should be documented with Rustdoc . Please follow the conventions as detailed below for modules, structs, enums, and functions. The single line is used as a preview when navigating Rustdoc. As an example, see the 'Structs' and 'Enums' sections in the collections Rustdoc. /// [Single line] One line summary description /// /// [Longer description] Multiple lines, inline code /// examples, invariants, purpose, usage, etc. [ Attributes ] If attributes exist , add after Rustdoc Example below: /// Represents (x, y) of a 2-dimensional grid /// /// A line is defined by 2 instances. /// A plane is defined by 3 instances. #[repr(C)] struct Point { x : i32 , y : i32 , } Testing \u00b6 Unit tests $ make test Stateless tests $ cd tests $ ./fuse-test","title":"Coding Guidelines"},{"location":"development/coding-guidelines/#code-formatting","text":"All code formatting is enforced with rustfmt with a project-specific configuration. Below is an example command: $ make fmt","title":"Code formatting"},{"location":"development/coding-guidelines/#code-analysis","text":"Clippy is used to catch common mistakes and is run as a part of continuous integration. Before submitting your code for review, you can run lint: $ make lint","title":"Code analysis"},{"location":"development/coding-guidelines/#code-documentation","text":"Any public fields, functions, and methods should be documented with Rustdoc . Please follow the conventions as detailed below for modules, structs, enums, and functions. The single line is used as a preview when navigating Rustdoc. As an example, see the 'Structs' and 'Enums' sections in the collections Rustdoc. /// [Single line] One line summary description /// /// [Longer description] Multiple lines, inline code /// examples, invariants, purpose, usage, etc. [ Attributes ] If attributes exist , add after Rustdoc Example below: /// Represents (x, y) of a 2-dimensional grid /// /// A line is defined by 2 instances. /// A plane is defined by 3 instances. #[repr(C)] struct Point { x : i32 , y : i32 , }","title":"Code documentation"},{"location":"development/coding-guidelines/#testing","text":"Unit tests $ make test Stateless tests $ cd tests $ ./fuse-test","title":"Testing"},{"location":"development/contributing/","text":"Contributing to Datafuse \u00b6 Datafuse is an open project, and you can contribute to it in many ways. You can help with ideas, code, or documentation. We appreciate any efforts that help us to make the project better. Our goal is to make contributing to the Datafuse project easy and transparent. Thank you. Notes Once the code been merged, your name will be stoned in the system.contributors table forever. SELECT * FROM system.contributors Contributing \u00b6 To contribute to Datafuse, ensure that you have the latest version of the codebase, run the following: $ git clone https://github.com/datafuselabs/datafuse $ cd datafuse $ make setup $ make test Coding Guidelines \u00b6 For detailed guidance on how to contribute to the codebase refer to Coding Guidelines . Documentation \u00b6 All developer documentation is published on the Datafuse developer site, datafuse.rs . Pull Requests \u00b6 To submit your pull request: Fork the datafuse repo and create your branch from master . Open an regular issue for binding the pull request. Submit a draft pull requests , tag your work in progress. If you have added code that should be tested, add unit tests. Verify and ensure that the test suites passes, make test . Make sure your code passes both linters, make lint . Change the status to \u201cReady for review\u201d. Watch out the replies from the @datafuse-bots , she will be your guide. Code of Conduct \u00b6 Please refer to the Code of Conduct , which describes the expectations for interactions within the community. Issues \u00b6 Datafuse uses GitHub issues to track bugs. Please include necessary information and instructions to reproduce your issue.","title":"Contributing"},{"location":"development/contributing/#contributing-to-datafuse","text":"Datafuse is an open project, and you can contribute to it in many ways. You can help with ideas, code, or documentation. We appreciate any efforts that help us to make the project better. Our goal is to make contributing to the Datafuse project easy and transparent. Thank you. Notes Once the code been merged, your name will be stoned in the system.contributors table forever. SELECT * FROM system.contributors","title":"Contributing to Datafuse"},{"location":"development/contributing/#contributing","text":"To contribute to Datafuse, ensure that you have the latest version of the codebase, run the following: $ git clone https://github.com/datafuselabs/datafuse $ cd datafuse $ make setup $ make test","title":"Contributing"},{"location":"development/contributing/#coding-guidelines","text":"For detailed guidance on how to contribute to the codebase refer to Coding Guidelines .","title":"Coding Guidelines"},{"location":"development/contributing/#documentation","text":"All developer documentation is published on the Datafuse developer site, datafuse.rs .","title":"Documentation"},{"location":"development/contributing/#pull-requests","text":"To submit your pull request: Fork the datafuse repo and create your branch from master . Open an regular issue for binding the pull request. Submit a draft pull requests , tag your work in progress. If you have added code that should be tested, add unit tests. Verify and ensure that the test suites passes, make test . Make sure your code passes both linters, make lint . Change the status to \u201cReady for review\u201d. Watch out the replies from the @datafuse-bots , she will be your guide.","title":"Pull Requests"},{"location":"development/contributing/#code-of-conduct","text":"Please refer to the Code of Conduct , which describes the expectations for interactions within the community.","title":"Code of Conduct"},{"location":"development/contributing/#issues","text":"Datafuse uses GitHub issues to track bugs. Please include necessary information and instructions to reproduce your issue.","title":"Issues"},{"location":"development/how-to-profile/","text":"We use flamegraph-rs to profile Datafuse. How to use \u00b6 sudo apt install -y linux-tools-common linux-tools-generic cargo install flamegraph echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid make profile More to see flamegraph-rs","title":"How to profile Datafuse"},{"location":"development/how-to-profile/#how-to-use","text":"sudo apt install -y linux-tools-common linux-tools-generic cargo install flamegraph echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid make profile More to see flamegraph-rs","title":"How to use"},{"location":"overview/architecture/","text":"Architecture \u00b6 FuseQuery \u00b6 FuseQuery is a Distributed Query Engine(Inspired by ClickHouse) at scale FuseStore \u00b6 FuseStore is a Distributed Table Storage Engine(Inspired by ClickHouse MergeTree) at scale.","title":"Architecture"},{"location":"overview/architecture/#architecture","text":"","title":"Architecture"},{"location":"overview/architecture/#fusequery","text":"FuseQuery is a Distributed Query Engine(Inspired by ClickHouse) at scale","title":"FuseQuery"},{"location":"overview/architecture/#fusestore","text":"FuseStore is a Distributed Table Storage Engine(Inspired by ClickHouse MergeTree) at scale.","title":"FuseStore"},{"location":"overview/building-and-running/","text":"This document describes how to build and run FuseQuery as a distributed query engine. 1. Deploy \u00b6 Run with Docker(Recommended) $ docker pull datafusedev/fuse-query $ docker run --init --rm -p 3307:3307 datafusedev/fuse-query Release binary Download: datafuse/releases From source $ git clone https://github.com/datafuselabs/datafuse.git $ cd datafuse $ make setup $ make run 2. Client \u00b6 MySQL Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ mysql -h127.0.0.1 -P3307 mysql> SELECT avg(number) FROM numbers(1000000000); +-------------+ | avg(number) | +-------------+ | 499999999.5 | +-------------+ 1 row in set (0.05 sec) ClickHouse Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ clickhouse client datafuse :) SELECT avg(number) FROM numbers(1000000000); SELECT avg(number) FROM numbers(1000000000) Query id: 89e06fba-1d57-464d-bfb0-238df85a2e66 \u250c\u2500avg(number)\u2500\u2510 \u2502 499999999.5 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 1 rows in set. Elapsed: 0.062 sec. Processed 1.00 billion rows, 8.01 GB (16.16 billion rows/s., 129.38 GB/s.)","title":"Installation"},{"location":"overview/building-and-running/#1-deploy","text":"Run with Docker(Recommended) $ docker pull datafusedev/fuse-query $ docker run --init --rm -p 3307:3307 datafusedev/fuse-query Release binary Download: datafuse/releases From source $ git clone https://github.com/datafuselabs/datafuse.git $ cd datafuse $ make setup $ make run","title":"1. Deploy"},{"location":"overview/building-and-running/#2-client","text":"MySQL Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ mysql -h127.0.0.1 -P3307 mysql> SELECT avg(number) FROM numbers(1000000000); +-------------+ | avg(number) | +-------------+ | 499999999.5 | +-------------+ 1 row in set (0.05 sec) ClickHouse Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ clickhouse client datafuse :) SELECT avg(number) FROM numbers(1000000000); SELECT avg(number) FROM numbers(1000000000) Query id: 89e06fba-1d57-464d-bfb0-238df85a2e66 \u250c\u2500avg(number)\u2500\u2510 \u2502 499999999.5 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 1 rows in set. Elapsed: 0.062 sec. Processed 1.00 billion rows, 8.01 GB (16.16 billion rows/s., 129.38 GB/s.)","title":"2. Client"},{"location":"overview/performance/","text":"Note Memory SIMD-Vector processing performance only Dataset: 100,000,000,000 (100 Billion) Hardware: AMD Ryzen 7 PRO 4750U, 8 CPU Cores, 16 Threads Rust: rustc 1.53.0-nightly (673d0db5e 2021-03-23) Build with Link-time Optimization and Using CPU Specific Instructions ClickHouse server version 21.4.6 revision 54447 Query FuseQuery (v0.4.1) ClickHouse (v21.4.6) SELECT avg(number) FROM numbers_mt(100000000000) 3.87 s. (25.83 billion rows/s., 206.79 GB/s.) \u00d71.6 slow, (6.04 s.) (16.57 billion rows/s., 132.52 GB/s.) SELECT sum(number) FROM numbers_mt(100000000000) 4.86 s. (20.57 billion rows/s., 164.70 GB/s.) \u00d71.2 slow, (5.90 s.) (16.95 billion rows/s., 135.62 GB/s.) SELECT min(number) FROM numbers_mt(100000000000) 5.61 s. (17.82 billion rows/s., 142.65 GB/s.) \u00d72.3 slow, (13.05 s.) (7.66 billion rows/s., 61.26 GB/s.) SELECT max(number) FROM numbers_mt(100000000000) 5.61 s. (17.82 billion rows/s., 142.67 GB/s.) \u00d72.5 slow, (14.07 s.) (7.11 billion rows/s., 56.86 GB/s.) SELECT count(number) FROM numbers_mt(100000000000) 3.12 s. (32.03 billion rows/s., 256.48 GB/s.) \u00d71.2 slow, (3.71 s.) (26.93 billion rows/s., 215.43 GB/s.) SELECT sum(number+number+number) FROM numbers_mt(100000000000) 17.85 s. (5.60 billion rows/s., 44.85 GB/s.) \u00d716.9 slow, (233.71 s.) (427.87 million rows/s., 3.42 GB/s.) SELECT sum(number) / count(number) FROM numbers_mt(100000000000) 4.02 s. (24.86 billion rows/s., 199.10 GB/s.) \u00d72.4 slow, (9.70 s.) (10.31 billion rows/s., 82.52 GB/s.) SELECT sum(number) / count(number), max(number), min(number) FROM numbers_mt(100000000000) 9.60 s. (10.41 billion rows/s., 83.38 GB/s.) \u00d73.4 slow, (32.87 s.) (3.04 billion rows/s., 24.34 GB/s.) SELECT number FROM numbers_mt(10000000000) ORDER BY number DESC LIMIT 1000 5.34 s. (1.87 billion rows/s., 14.99 GB/s.) \u00d72.6 slow, (13.95 s.) (716.62 million rows/s., 5.73 GB/s.) SELECT max(number),sum(number) FROM numbers_mt(1000000000) GROUP BY number % 3, number % 4, number % 5 9.03 s. (110.71 million rows/s., 886.50 MB/s.) \u00d73.5 fast, (2.60 s.) (385.28 million rows/s., 3.08 GB/s.) Notes ClickHouse system.numbers_mt is 16-way parallelism processing, gist FuseQuery system.numbers_mt is 16-way parallelism processing, gist 100,000,000,000 records on laptop show Experience 100 billion performance on your laptop, talk is cheap just bench it","title":"Performance"},{"location":"policies/cla/","text":"Datafuse Labs, Inc. \u00b6 Contributor License Agreement \u00b6 Thank you for your interest in the open source project(s) managed by Datafuse Labs, Inc. (\"Datafuse Labs\"). In order to clarify the intellectual property license granted with Contributions from any person or entity, Datafuse Labs must have a Contributor License Agreement (\"CLA\") on file that has been entered into by each contributor, indicating agreement to the license terms below. This license is for your protection as a contributor as well as the protection of Datafuse Labs and its other contributors and users; it does not change your rights to use your own Contributions for any other purpose. 1. Definitions. \"You\" (or \"Your\") shall mean the copyright owner or legal entity authorized by the copyright owner that is entering into this CLA with Datafuse Labs. For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"Contribution\" shall mean any code, documentation or other original works of authorship, including any modifications or additions to an existing work, that are intentionally submitted by You to Datafuse Labs for inclusion in, or documentation of, any of the products owned or managed by Datafuse Labs (the \"Work\"). For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to Datafuse Labs or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Datafuse Labs for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\" 2. Grant of Copyright License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works. 3. Grant of Patent License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by You that are necessarily infringed by Your Contribution(s) alone or by combination of Your Contribution(s) with the Work to which such Contribution(s) were submitted. If any entity institutes patent litigation against You or any other entity (including a cross-claim or counterclaim in a lawsuit) alleging that Your Contribution, or the Work to which You have contributed, constitutes direct or contributory patent infringement, then any patent licenses granted to that entity under this CLA for that Contribution or Work shall terminate as of the date such litigation is filed. 4. Authority. You represent and warrant that You are legally entitled to grant the above license. If You are an individual and Your employer(s) has rights to intellectual property that You create that includes Your Contributions, You represent that You have received permission to make Contributions on behalf of that employer, that Your employer has waived such rights for Your Contributions to Datafuse Labs, or that Your employer has entered into a separate CLA with Datafuse Labs covering Your Contributions. If You are a Company,You represent further that each employee making a Contribution to Datafuse Labs under the Company's name is authorized to submit Contributions on behalf of the Company. 5. Original Works. You represent and warrant that each of Your Contributions is Your original creation (see section 7 for submissions on behalf of others). You represent and warrant that, to Your knowledge, none of Your Contributions infringe, violate, or misappropriate any third party intellectual property or other proprietary rights. 6. Disclaimer. You are not expected to provide support for Your Contributions, except to the extent You desire to provide support. You may provide support for free, for a fee, or not at all. UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING, EXCEPT FOR THE WARRANTIES SET FORTH ABOVE, YOU PROVIDE YOUR CONTRIBUTIONS ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. 7. Submissions on Behalf of Others. Should You wish to submit work that is not Your original creation, You may submit it to Datafuse Labs separately from any Contribution, identifying the complete details of its source and of any license or other restriction (including, but not limited to, related patents, trademarks, and license agreements) of which You are personally aware, and conspicuously marking the work as \"Submitted on behalf of a third-party: [named here]\". 8. Additional Facts/Circumstances. You agree to notify Datafuse Labs of any facts or circumstances of which You become aware that would make the above representations and warranties inaccurate in any respect. 9. Authorization. If You are entering into this CLA as a Company, You represent and warrant that the individual accepting this CLA is duly authorized to enter into this CLA on the Company's behalf.","title":"Contributor License Agreement"},{"location":"policies/cla/#datafuse-labs-inc","text":"","title":"Datafuse Labs, Inc."},{"location":"policies/cla/#contributor-license-agreement","text":"Thank you for your interest in the open source project(s) managed by Datafuse Labs, Inc. (\"Datafuse Labs\"). In order to clarify the intellectual property license granted with Contributions from any person or entity, Datafuse Labs must have a Contributor License Agreement (\"CLA\") on file that has been entered into by each contributor, indicating agreement to the license terms below. This license is for your protection as a contributor as well as the protection of Datafuse Labs and its other contributors and users; it does not change your rights to use your own Contributions for any other purpose. 1. Definitions. \"You\" (or \"Your\") shall mean the copyright owner or legal entity authorized by the copyright owner that is entering into this CLA with Datafuse Labs. For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"Contribution\" shall mean any code, documentation or other original works of authorship, including any modifications or additions to an existing work, that are intentionally submitted by You to Datafuse Labs for inclusion in, or documentation of, any of the products owned or managed by Datafuse Labs (the \"Work\"). For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to Datafuse Labs or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Datafuse Labs for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\" 2. Grant of Copyright License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works. 3. Grant of Patent License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by You that are necessarily infringed by Your Contribution(s) alone or by combination of Your Contribution(s) with the Work to which such Contribution(s) were submitted. If any entity institutes patent litigation against You or any other entity (including a cross-claim or counterclaim in a lawsuit) alleging that Your Contribution, or the Work to which You have contributed, constitutes direct or contributory patent infringement, then any patent licenses granted to that entity under this CLA for that Contribution or Work shall terminate as of the date such litigation is filed. 4. Authority. You represent and warrant that You are legally entitled to grant the above license. If You are an individual and Your employer(s) has rights to intellectual property that You create that includes Your Contributions, You represent that You have received permission to make Contributions on behalf of that employer, that Your employer has waived such rights for Your Contributions to Datafuse Labs, or that Your employer has entered into a separate CLA with Datafuse Labs covering Your Contributions. If You are a Company,You represent further that each employee making a Contribution to Datafuse Labs under the Company's name is authorized to submit Contributions on behalf of the Company. 5. Original Works. You represent and warrant that each of Your Contributions is Your original creation (see section 7 for submissions on behalf of others). You represent and warrant that, to Your knowledge, none of Your Contributions infringe, violate, or misappropriate any third party intellectual property or other proprietary rights. 6. Disclaimer. You are not expected to provide support for Your Contributions, except to the extent You desire to provide support. You may provide support for free, for a fee, or not at all. UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING, EXCEPT FOR THE WARRANTIES SET FORTH ABOVE, YOU PROVIDE YOUR CONTRIBUTIONS ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. 7. Submissions on Behalf of Others. Should You wish to submit work that is not Your original creation, You may submit it to Datafuse Labs separately from any Contribution, identifying the complete details of its source and of any license or other restriction (including, but not limited to, related patents, trademarks, and license agreements) of which You are personally aware, and conspicuously marking the work as \"Submitted on behalf of a third-party: [named here]\". 8. Additional Facts/Circumstances. You agree to notify Datafuse Labs of any facts or circumstances of which You become aware that would make the above representations and warranties inaccurate in any respect. 9. Authorization. If You are entering into this CLA as a Company, You represent and warrant that the individual accepting this CLA is duly authorized to enter into this CLA on the Company's behalf.","title":"Contributor License Agreement"},{"location":"policies/code-of-conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at opensource@datafuselabs.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Contributor Covenant Code of Conduct"},{"location":"policies/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"policies/code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"policies/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"policies/code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"policies/code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"policies/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at opensource@datafuselabs.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"policies/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"policies/notice/","text":"Datafuse includes some code from arrow/datafusion \u00b6 We include the text of the original license below: Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Datafuse includes some code from [arrow/datafusion](https://github.com/apache/arrow/tree/master/rust/datafusion)"},{"location":"policies/notice/#datafuse-includes-some-code-from-arrowdatafusion","text":"We include the text of the original license below: Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Datafuse includes some code from arrow/datafusion"},{"location":"rfcs/query/2021-05-01-join-framework-design/","text":"Proposal: Join framework \u00b6 Background \u00b6 Join is one of the major features in SQL. Meanwhile, it's the most complicated part either. Thus in this section, we will make a brief introduction to types of join semantics and join algorithms. Generally, join can be categorized as following types by semantic: INNER JOIN : return all tuples satisfy the join condition LEFT OUTER JOIN : return all tuples satisfy the join condition and the rows from left table for which no row from right table satisfies the join condition RIGHT OUTER JOIN : return all tuples satisfy the join condition and the rows from right table for which no row from left table satisfies the join condition FULL OUTER JOIN : return all tuples satisfy the join condition and the rows from a table for which no row from other table satisfies the join condition CROSS JOIN : cartesian product of joined tables Besides, IN , EXISTS , NOT IN , NOT EXISTS expressions can be implemented by semi-join and anti-join (known as subquery). There are three kinds of common join algorithms: Nested-loop join Hash join Sort-merge join Nested-loop join is the basic join algorithm, it can be described as following pseudo code: // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable: for r <- innerTable: if condition(r, s) == true: insert(result, combine(r, s)) Before introducing hash join, we introduce the definition of equi join here. A equi join is join whose join condition is an equation(e.g. r.a == s.a ). For the joins whose join condition is not an equation, we call them non-equi join Hash join can only work with equi join. It can be described as two phase: build phase and probe phase . As inner table and outer table of nested-loop join, hash join will choose a table as build side and another table as probe side . The pseudo code of hash join: // R\u22c8S var build = R var probe = S var hashTable var result // Build phase for r <- build: var key = hash(r, condition) insert(hashTable, key, r) // Probe phase for s <- probe: var key = hash(s, condition) if exists(hashTable, key): var r = get(hashTable, key) insert(result, combine(r, s)) Sort-merge join will sort the joined tables if they are not sorted by join key, and then merge them like merge sort. Generally, a sort-merge join can only work with equi-join either, but it exists a band join optimization that can make sort-merge join work with some specific non-equi join. We won't talk about this here since it's a little bit out of scope. Join framework \u00b6 To implement join, we have several parts of work to be done: Support parse join statement into logical plan Support bind column reference for joined tables Support some basic heuristic optimization(e.g. outer join elimination, subquery elimination) and join reorder with choosing implementation Support some join algorithms(local execution for now but design for distributed execution) Parser & Planner \u00b6 According to ANSI-SQL specification, joins are defined in FROM clause. Besides, subquery in other clauses can be translated to join(correlated subquery will be translated to semi join or anti join) in some cases. After parsing SQL string into AST, we will build logical plan from AST with PlanParser . Following bnf definition is a simplified ANSI-SQL specification of FROM clause: < from clause > ::= FROM < table reference list > < table reference list > ::= < table reference > [ { < comma > < table reference > }... ] < table reference > ::= < table primary or joined table > < table primary or joined table > ::= < table primary > | < joined table > < table primary > ::= < table or query name > [ [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] ] | < derived table > [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] | < left paren > < joined table > < right paren > < joined table > ::= < cross join > | < qualified join > | < natural join > < cross join > ::= < table reference > CROSS JOIN < table primary > < qualified join > ::= < table reference > [ < join type > ] JOIN < table reference > < join specification > < natural join > ::= < table reference > NATURAL [ < join type > ] JOIN < table primary > < join specification > ::= < join condition > | < named columns join > < join condition > ::= ON < search condition > < named columns join > ::= USING < left paren > < join column list > < right paren > < join type > ::= INNER | < outer join type > [ OUTER ] < outer join type > ::= LEFT | RIGHT | FULL < join column list > ::= < column name list > <table reference> concated with <comma> are cross joined. And it's possible to find some conjunctions in WHERE clause as their join conditions, that is rewriting cross join into inner join. There are many queries organized in this way that doesn't explicitly specify join condition, for example TPCH query set. sqlparser library can parse a SQL string into AST. Joins are organized as a tree structure. There are following kinds of join trees: Left deep tree Right deep tree Bushy tree In left deep tree, every join node's right child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join d / \\ join c / \\ a b */ In right deep tree, every join node's left child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ a join / \\ b join / \\ c d */ In bushy tree, all children of every join node can be either result of join or table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join join / \\ / \\ a b c d */ Most of join s can be represented as left deep tree, which is easier to optimize. We can rewrite some joins to left deep tree during parsing phase. Here's an example of sqlparser AST, the comment part is simplified AST debug string: SELECT * FROM a , b NATURAL JOIN c , d ; /* Query { with: None, body: Select( Select { projection: [Wildcard], from: [ TableWithJoins { relation: Table { name: \"a\", }, joins: [] }, TableWithJoins { relation: Table { name: \"b\", }, joins: [ Join { relation: Table { name: \"c\", }, join_operator: Inner(Natural) } ] }, TableWithJoins { relation: Table { name: \"d\", }, joins: [] } ], } ), } */ The AST above can be directly represented as a bushy tree: join / \\ join d / \\ a join / \\ b c This bushy tree is equivalent to the following left deep tree so we can rewrite it in parsing phase: join / \\ join d / \\ join c / \\ a b After rewriting AST to left deep tree, we will bind the AST to concrete tables and columns with catalog. During binding, semantic checking is necessary(e.g. check whether column name is ambiguous). To implement semantic checking and simplify the binding process, we introduce Scope to represent context of each query block. It will record information about available columns in current context and which table they belong to. Columns from a parent Scope is visible to all of its children Scope . struct Scope { pub parent : Arc < Scope > , pub columns : Vec < ColumnRef > } Here's an example to explain how Scope works: CREATE TABLE t0 ( a INT ); CREATE TABLE t1 ( b INT ); CREATE TABLE t2 ( c INT ); SELECT * FROM t0 , ( SELECT b , c , c + 1 AS d FROM t1 , t2 ) t ; /* Scope root: [t0.a, t.b, t.c, t.d] | \\ | Scope t0: [a] | Scope t: [t1.b, t2.c, d] | \\ | Scope t1: [b] | Scope t2: [c] */ Since it may exist different column with same name after join, we should identify ColumnRef with a unique ColumnID . Meanwhile, correlation names are ensured to be unique, it's fine to identify them with name strings. struct ColumnRef { pub id : ColumnID , pub column_name : String , pub table_name : String } With unique ColumnID , we can check whether a query is ambiguous or not and keep their original name at the same time. For planner, we will add a variant Join for PlanNode to represent join operator: enum PlanNode { .. . Join ( JoinPlan ) } enum JoinType { Inner , LeftOuter , RightOuter , FullOuter , Cross } struct JoinPlan { pub join_type : JoinType , pub join_conditions : Vec < ExpressionPlan > , // Conjunctions of join condition pub left_child : Arc < PlanNode > , pub right_child : Arc < PlanNode > } Here's a problem that fuse-query uses arrow::datatypes::Schema to represent data schema, while arrow::datatypes::Schema doesn't support identify columns with ColumnID natively. I suggest to introduce an internal DataSchema struct to represent data schema in fuse-query, which can store more information and can be converted to arrow::datatypes::Schema naturally. struct DataSchema { pub columns : Vec < Arc < Column >> } struct Column { pub column_id : ColumnID , pub column_name : String , pub data_type : DataType , pub is_nullable : bool } Optimizer \u00b6 There are two kinds of optimization to be done: Heuristic optimization Cost-based optimization The heuristic optimization( RBO , aka rule-based optimization), is the optimization which can always reduce cost of a query. Since there are too many heuristic rules, we won't discuss this here. The cost-based optimization uses statistic information to calculate the cost of a query. With exploring framework(e.g. Volcano optimizer, Cascades optimizer), it can choose the best execution plan. Optimizer is the most complicated part in a SQL engine, we'd better only support limited heuristic optimization at the beginning. TODO: list common heuristic rules Execution \u00b6 As we discussed in section Background , join algorithms can be categorized into three kinds: Nested-loop join Hash join Sort-merge join Besides, there are two kinds of distributed join algorithms: Broadcast join Repartition join(aka shuffle join) We won't talk about detail of distributed join algorithms here, but we still need to consider about them. Different join algorithms have advantage on different scenarios. Nested-loop join is effective if the amount of data is relatively small. With vectorized execution model, it's natural to implement block nested-loop join, which is a refined nested-loop join algorithm. Another advantage of nested-loop join is it can work with non-equi join condition. Hash join is effective if one of the joined table is small and the other one is large. Since distributed join algorithm will always produce small tables(by partition), it fits hash join a lot. Meanwhile, vectorized hash join algorithm has been introduced by Marcin Zucowski (Co-founder of Snowflake, Phd of CWI). The disadvantage of hash join is that hash join will consume more memory than other join algorithms, and it only supports equi join. Sort-merge join is effective if inputs are sorted, while this is rarely happened. The comparison above is much biased, in fact it can hardly say that which algorithm is better. IMO, we can implement hash join and nested-loop join first since they are more common. Since we don't have infrastructure(planner, optimizer) for choosing join algorithm for now, I suggest to only implement block nested-loop join at present so we can build a complete prototype. We'are going to introduce a vectorized block nested-loop join algorithm. Pseudo code of naive nested-loop join has been introduced in Background section. As we know, nested-loop join will fetch only one row from outer table in each loop, which doen't have good locality. Block nested-loop join is a nested-loop join that will fetch a block of data in each loop. Here we introduce the naive block nested-loop join. // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable.fetchBlock(): for r <- innerTable.fetchBlock(): buffer = conditionEvalBlock(s, r) for row <- buffer: insert(result, row) In vetorized execution, we can use a bit map to indicate whether a row should be return to result set or not. Then we can materialize the result later. For example, assume we have following SQL query: CREATE TABLE t ( a int , b int ); CREATE TABLE t1 ( b int , c int ); -- insert some rows SELECT a , b , c FROM t INNER JOIN t1 ON t . b = t1 . b ; The execution plan of this query should look like: Join (t.b = t1.b) -> TableScan t -> TableScan t1 If we use the vectorized block nested-loop join algorithm introduced above, the pseudo code should look like: var leftChild: BlockStream = scan(t) var rightChild: BlockStream = scan(t1) var condition: Expression = equal(column(t.b), column(t1.b)) var result for l <- leftChild: for r <- rightChild: buffer = mergeBlock(l, r) var bitMap: Array[boolean] = condition.eval(buffer) buffer.insertColumn(bitMap) result.insertBlock(buffer) materialize(result) In fuse-query, we can add a NestedLoopJoinTransform to implement vectorized block nested-loop join.","title":"FuseQuery Join"},{"location":"rfcs/query/2021-05-01-join-framework-design/#proposal-join-framework","text":"","title":"Proposal: Join framework"},{"location":"rfcs/query/2021-05-01-join-framework-design/#background","text":"Join is one of the major features in SQL. Meanwhile, it's the most complicated part either. Thus in this section, we will make a brief introduction to types of join semantics and join algorithms. Generally, join can be categorized as following types by semantic: INNER JOIN : return all tuples satisfy the join condition LEFT OUTER JOIN : return all tuples satisfy the join condition and the rows from left table for which no row from right table satisfies the join condition RIGHT OUTER JOIN : return all tuples satisfy the join condition and the rows from right table for which no row from left table satisfies the join condition FULL OUTER JOIN : return all tuples satisfy the join condition and the rows from a table for which no row from other table satisfies the join condition CROSS JOIN : cartesian product of joined tables Besides, IN , EXISTS , NOT IN , NOT EXISTS expressions can be implemented by semi-join and anti-join (known as subquery). There are three kinds of common join algorithms: Nested-loop join Hash join Sort-merge join Nested-loop join is the basic join algorithm, it can be described as following pseudo code: // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable: for r <- innerTable: if condition(r, s) == true: insert(result, combine(r, s)) Before introducing hash join, we introduce the definition of equi join here. A equi join is join whose join condition is an equation(e.g. r.a == s.a ). For the joins whose join condition is not an equation, we call them non-equi join Hash join can only work with equi join. It can be described as two phase: build phase and probe phase . As inner table and outer table of nested-loop join, hash join will choose a table as build side and another table as probe side . The pseudo code of hash join: // R\u22c8S var build = R var probe = S var hashTable var result // Build phase for r <- build: var key = hash(r, condition) insert(hashTable, key, r) // Probe phase for s <- probe: var key = hash(s, condition) if exists(hashTable, key): var r = get(hashTable, key) insert(result, combine(r, s)) Sort-merge join will sort the joined tables if they are not sorted by join key, and then merge them like merge sort. Generally, a sort-merge join can only work with equi-join either, but it exists a band join optimization that can make sort-merge join work with some specific non-equi join. We won't talk about this here since it's a little bit out of scope.","title":"Background"},{"location":"rfcs/query/2021-05-01-join-framework-design/#join-framework","text":"To implement join, we have several parts of work to be done: Support parse join statement into logical plan Support bind column reference for joined tables Support some basic heuristic optimization(e.g. outer join elimination, subquery elimination) and join reorder with choosing implementation Support some join algorithms(local execution for now but design for distributed execution)","title":"Join framework"},{"location":"rfcs/query/2021-05-01-join-framework-design/#parser-planner","text":"According to ANSI-SQL specification, joins are defined in FROM clause. Besides, subquery in other clauses can be translated to join(correlated subquery will be translated to semi join or anti join) in some cases. After parsing SQL string into AST, we will build logical plan from AST with PlanParser . Following bnf definition is a simplified ANSI-SQL specification of FROM clause: < from clause > ::= FROM < table reference list > < table reference list > ::= < table reference > [ { < comma > < table reference > }... ] < table reference > ::= < table primary or joined table > < table primary or joined table > ::= < table primary > | < joined table > < table primary > ::= < table or query name > [ [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] ] | < derived table > [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] | < left paren > < joined table > < right paren > < joined table > ::= < cross join > | < qualified join > | < natural join > < cross join > ::= < table reference > CROSS JOIN < table primary > < qualified join > ::= < table reference > [ < join type > ] JOIN < table reference > < join specification > < natural join > ::= < table reference > NATURAL [ < join type > ] JOIN < table primary > < join specification > ::= < join condition > | < named columns join > < join condition > ::= ON < search condition > < named columns join > ::= USING < left paren > < join column list > < right paren > < join type > ::= INNER | < outer join type > [ OUTER ] < outer join type > ::= LEFT | RIGHT | FULL < join column list > ::= < column name list > <table reference> concated with <comma> are cross joined. And it's possible to find some conjunctions in WHERE clause as their join conditions, that is rewriting cross join into inner join. There are many queries organized in this way that doesn't explicitly specify join condition, for example TPCH query set. sqlparser library can parse a SQL string into AST. Joins are organized as a tree structure. There are following kinds of join trees: Left deep tree Right deep tree Bushy tree In left deep tree, every join node's right child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join d / \\ join c / \\ a b */ In right deep tree, every join node's left child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ a join / \\ b join / \\ c d */ In bushy tree, all children of every join node can be either result of join or table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join join / \\ / \\ a b c d */ Most of join s can be represented as left deep tree, which is easier to optimize. We can rewrite some joins to left deep tree during parsing phase. Here's an example of sqlparser AST, the comment part is simplified AST debug string: SELECT * FROM a , b NATURAL JOIN c , d ; /* Query { with: None, body: Select( Select { projection: [Wildcard], from: [ TableWithJoins { relation: Table { name: \"a\", }, joins: [] }, TableWithJoins { relation: Table { name: \"b\", }, joins: [ Join { relation: Table { name: \"c\", }, join_operator: Inner(Natural) } ] }, TableWithJoins { relation: Table { name: \"d\", }, joins: [] } ], } ), } */ The AST above can be directly represented as a bushy tree: join / \\ join d / \\ a join / \\ b c This bushy tree is equivalent to the following left deep tree so we can rewrite it in parsing phase: join / \\ join d / \\ join c / \\ a b After rewriting AST to left deep tree, we will bind the AST to concrete tables and columns with catalog. During binding, semantic checking is necessary(e.g. check whether column name is ambiguous). To implement semantic checking and simplify the binding process, we introduce Scope to represent context of each query block. It will record information about available columns in current context and which table they belong to. Columns from a parent Scope is visible to all of its children Scope . struct Scope { pub parent : Arc < Scope > , pub columns : Vec < ColumnRef > } Here's an example to explain how Scope works: CREATE TABLE t0 ( a INT ); CREATE TABLE t1 ( b INT ); CREATE TABLE t2 ( c INT ); SELECT * FROM t0 , ( SELECT b , c , c + 1 AS d FROM t1 , t2 ) t ; /* Scope root: [t0.a, t.b, t.c, t.d] | \\ | Scope t0: [a] | Scope t: [t1.b, t2.c, d] | \\ | Scope t1: [b] | Scope t2: [c] */ Since it may exist different column with same name after join, we should identify ColumnRef with a unique ColumnID . Meanwhile, correlation names are ensured to be unique, it's fine to identify them with name strings. struct ColumnRef { pub id : ColumnID , pub column_name : String , pub table_name : String } With unique ColumnID , we can check whether a query is ambiguous or not and keep their original name at the same time. For planner, we will add a variant Join for PlanNode to represent join operator: enum PlanNode { .. . Join ( JoinPlan ) } enum JoinType { Inner , LeftOuter , RightOuter , FullOuter , Cross } struct JoinPlan { pub join_type : JoinType , pub join_conditions : Vec < ExpressionPlan > , // Conjunctions of join condition pub left_child : Arc < PlanNode > , pub right_child : Arc < PlanNode > } Here's a problem that fuse-query uses arrow::datatypes::Schema to represent data schema, while arrow::datatypes::Schema doesn't support identify columns with ColumnID natively. I suggest to introduce an internal DataSchema struct to represent data schema in fuse-query, which can store more information and can be converted to arrow::datatypes::Schema naturally. struct DataSchema { pub columns : Vec < Arc < Column >> } struct Column { pub column_id : ColumnID , pub column_name : String , pub data_type : DataType , pub is_nullable : bool }","title":"Parser &amp; Planner"},{"location":"rfcs/query/2021-05-01-join-framework-design/#optimizer","text":"There are two kinds of optimization to be done: Heuristic optimization Cost-based optimization The heuristic optimization( RBO , aka rule-based optimization), is the optimization which can always reduce cost of a query. Since there are too many heuristic rules, we won't discuss this here. The cost-based optimization uses statistic information to calculate the cost of a query. With exploring framework(e.g. Volcano optimizer, Cascades optimizer), it can choose the best execution plan. Optimizer is the most complicated part in a SQL engine, we'd better only support limited heuristic optimization at the beginning. TODO: list common heuristic rules","title":"Optimizer"},{"location":"rfcs/query/2021-05-01-join-framework-design/#execution","text":"As we discussed in section Background , join algorithms can be categorized into three kinds: Nested-loop join Hash join Sort-merge join Besides, there are two kinds of distributed join algorithms: Broadcast join Repartition join(aka shuffle join) We won't talk about detail of distributed join algorithms here, but we still need to consider about them. Different join algorithms have advantage on different scenarios. Nested-loop join is effective if the amount of data is relatively small. With vectorized execution model, it's natural to implement block nested-loop join, which is a refined nested-loop join algorithm. Another advantage of nested-loop join is it can work with non-equi join condition. Hash join is effective if one of the joined table is small and the other one is large. Since distributed join algorithm will always produce small tables(by partition), it fits hash join a lot. Meanwhile, vectorized hash join algorithm has been introduced by Marcin Zucowski (Co-founder of Snowflake, Phd of CWI). The disadvantage of hash join is that hash join will consume more memory than other join algorithms, and it only supports equi join. Sort-merge join is effective if inputs are sorted, while this is rarely happened. The comparison above is much biased, in fact it can hardly say that which algorithm is better. IMO, we can implement hash join and nested-loop join first since they are more common. Since we don't have infrastructure(planner, optimizer) for choosing join algorithm for now, I suggest to only implement block nested-loop join at present so we can build a complete prototype. We'are going to introduce a vectorized block nested-loop join algorithm. Pseudo code of naive nested-loop join has been introduced in Background section. As we know, nested-loop join will fetch only one row from outer table in each loop, which doen't have good locality. Block nested-loop join is a nested-loop join that will fetch a block of data in each loop. Here we introduce the naive block nested-loop join. // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable.fetchBlock(): for r <- innerTable.fetchBlock(): buffer = conditionEvalBlock(s, r) for row <- buffer: insert(result, row) In vetorized execution, we can use a bit map to indicate whether a row should be return to result set or not. Then we can materialize the result later. For example, assume we have following SQL query: CREATE TABLE t ( a int , b int ); CREATE TABLE t1 ( b int , c int ); -- insert some rows SELECT a , b , c FROM t INNER JOIN t1 ON t . b = t1 . b ; The execution plan of this query should look like: Join (t.b = t1.b) -> TableScan t -> TableScan t1 If we use the vectorized block nested-loop join algorithm introduced above, the pseudo code should look like: var leftChild: BlockStream = scan(t) var rightChild: BlockStream = scan(t1) var condition: Expression = equal(column(t.b), column(t1.b)) var result for l <- leftChild: for r <- rightChild: buffer = mergeBlock(l, r) var bitMap: Array[boolean] = condition.eval(buffer) buffer.insertColumn(bitMap) result.insertBlock(buffer) materialize(result) In fuse-query, we can add a NestedLoopJoinTransform to implement vectorized block nested-loop join.","title":"Execution"},{"location":"rfcs/query/2021-05-22-plan-expression/","text":"Expression and plan builder \u00b6 Summary \u00b6 Logic plan and expression play a big role throughout the life cycle of SQL query. This doc is intended to explain the new design of expressions and plan builder. Expression \u00b6 Alias Expression \u00b6 Aliasing is useful in SQL, we can alias a complex expression as a short alias name. Such as: select a + 3 as b . In the standard SQL protocol, aliasing can work in: Group By, eg: select a + 3 as b, count(1) from table group by b Having, eg: select a + 3 as b, count(1) as c from table group by b having c > 0 Order By: eg: select a + 3 as b from table order by b Notes ClickHouse has extended the usage of expression alias, it can be work in: recursive alias expression: eg: select a + 1 as b, b + 1 as c filter: eg: select a + 1 as b, b + 1 as c from table where c > 0 Note Currently we do not support clickhouse style alias expression. It can be implemented later. For expression alias, we only handle it at last, in projection stage. But We have to replace the alias of the expression as early as possible to prevent ambiguity later. Eg: select number + 1 as c, sum(number) from numbers(10) group by c having c > 3 order by c limit 10 Firstly, we can scan all the alias expressions from projection ASTs. c ---> (number + 1) Then we replaced the alias into the corresponding expression in having , order by , group by clause. So the query will be: select number + 1 as c, sum(number) from numbers(10) group by (number + 1) having (number + 1) > 3 order by (number + 1) limit 10 At last, when the query is finished, we apply the projection to rename the column (number+1) to c Let's take a look at the explain result of this query: | Limit: 10 Projection: (number + 1) as c:UInt64, sum(number):UInt64 Sort: (number + 1):UInt64 Having: ((number + 1) > 3) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum(number)]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum(number)]] Expression: (number + 1):UInt64, number:UInt64 (Before GroupBy) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] We can see we do not need to care about aliasing until the projection, so it will be very convenient to apply other expressions. Materialized Expression \u00b6 Materialized expression processing is that we can rebase the expression as a ExpressionColumn if the same expression is already processed upstream. Eg: select number + 1 as c, sum(number) as d group by c having number + 1 > 3 order by d desc After aliases replacement, we will know that order by is sum(number) , but sum(number) is already processed during the aggregating stage, so we can rebase the order by expression SortExpression { ... } to Column(\"sum(number)\") , this could remove useless calculation of same expressions. So number + 1 in having can also apply to rebase the expression. Expression Functions \u00b6 There are many kinds of expression functions. ScalarFunctions, One-to-one calculation process, the result rows is same as the input rows. eg: select database() AggregateFunctions, Many-to-one calculation process, eg: select sum(number) BinaryFunctions, a special kind of \u00b7ScalarFunctions\u00b7 eg: select 1 + 2 ... For ScalarFunctions, we really don't care about the whole block, we just care about the columns involved by the arguments. sum(number) just care about the Column which named number . And the result is also a column, so we have the virtual method in IFunction is: fn eval ( & self , columns : & [ DataColumnarValue ], _input_rows : usize ) -> Result < DataColumnarValue > ; For AggregateFunctions, we should keep the state in the corresponding function instance to apply the two-level merge, we have the following virtual method in IAggregateFunction : fn accumulate ( & mut self , columns : & [ DataColumnarValue ], _input_rows : usize ) -> Result < () > ; fn accumulate_result ( & self ) -> Result < Vec < DataValue >> ; fn merge ( & mut self , _states : & [ DataValue ]) -> Result < () > ; fn merge_result ( & self ) -> Result < DataValue > ; The process is accumulate (apply data to the function) \u2192 accumulate_result (to get the current state) \u2192 merge (merge current state from other state) ---> merge_result (to get the final result value) ps: We don't store the arguments types and arguments names in functions, we can store them later if we need. Column \u00b6 Block is the unit of data passed between streams for pipeline processing, while Column is the unit of data passed between expressions. So in the view of expression(functions, literal, ...), everything is Column , we have DataColumnarValue to represent a column. #[derive(Clone, Debug)] pub enum DataColumnarValue { // Array of values. Array ( DataArrayRef ), // A Single value. Constant ( DataValue , usize ) } DataColumnarValue::Constant is like ConstantColumn in ClickHouse . Note: We don't have ScalarValue , because it can be known as Constant(DataValue, 1) , and there is DataValue struct. Expression chain and expression executor \u00b6 Currently, we can collect the inner expression from expressions to build ExpressionChain. This could be done by Depth-first-search visiting. ExpressionFunction: number + (number + 1) will be : [ ExpressionColumn(number), ExpressionColumn(number), ExpressionLiteral(1), ExpressionBinary('+', 'number', '1'), ExpressionBinary('+', 'number', '(number + 1)') ] . We have the ExpressionExecutor the execute the expression chain, during the execution, we don't need to care about the kind of the arguments. We just consider them as ColumnExpression from upstream, so we just fetch the column number and the column (number + 1) from the block. Plan Builder \u00b6 None aggregation query \u00b6 This is for queries without group by and aggregate functions . Eg: explain select number + 1 as b from numbers(10) where number + 1 > 3 order by number + 3 | explain | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: (number + 1) as b:UInt64 Sort: (number + 3):UInt64 Expression: (number + 1):UInt64, (number + 3):UInt64 (Before OrderBy) Filter: ((number + 1) > 3) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.02 sec) The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 1) > 3 , the schema keeps the same, schema \u2192 [number] Expression: we will collect expressions from order by and having clauses to apply the expression, schema \u2192 [number, number + 1, number + 3] Sort: since we already have the number + 1 in the input plan, so the sorting will consider number + 1 as ColumnExpression , schema \u2192 [number, number + 1, number + 3] Projection: applying the aliases and projection the columns, schema \u2192 [b] Aggregation query \u00b6 To build Aggregation query, there will be more complex than the previous one. Eg: explain select number + 1 as b, sum(number + 2 ) + 4 as c from numbers(10) where number + 3 > 0 group by number + 1 having c > 3 and sum(number + 4) + 1 > 4 order by sum(number + 5) + 1; | Projection: (number + 1) as b:UInt64, (sum((number + 2)) + 4) as c:UInt64 Sort: sum((number + 5)):UInt64 Having: (((sum((number + 2)) + 4) > 3) AND (sum((number + 4)) > 0)) Expression: (number + 1):UInt64, (sum((number + 2)) + 4):UInt64, sum((number + 5)):UInt64 (Before OrderBy) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] Expression: (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Filter: ((number + 3) > 0) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 3) > 0 , the schema keeps the same, schema \u2192 [number] Expression: Before group by (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Before GroupBy, We must visit all the expression in projections , having , group by to collect the expressions and aggregate functions, schema \u2192 [number, number + 1, number + 2, number + 4, number + 5] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] , note that: the expressions are already materialized in upstream, so we just conside all the arguments as columns. AggregatorFinal, schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4))] Expression: schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4)), sum((number + 2)) + 4, sum((number + 5)) + 1] Sort: the schema keeps the same Projection: schema \u2192 b, c","title":"FuseQuery Expression"},{"location":"rfcs/query/2021-05-22-plan-expression/#expression-and-plan-builder","text":"","title":"Expression and plan builder"},{"location":"rfcs/query/2021-05-22-plan-expression/#summary","text":"Logic plan and expression play a big role throughout the life cycle of SQL query. This doc is intended to explain the new design of expressions and plan builder.","title":"Summary"},{"location":"rfcs/query/2021-05-22-plan-expression/#expression","text":"","title":"Expression"},{"location":"rfcs/query/2021-05-22-plan-expression/#alias-expression","text":"Aliasing is useful in SQL, we can alias a complex expression as a short alias name. Such as: select a + 3 as b . In the standard SQL protocol, aliasing can work in: Group By, eg: select a + 3 as b, count(1) from table group by b Having, eg: select a + 3 as b, count(1) as c from table group by b having c > 0 Order By: eg: select a + 3 as b from table order by b Notes ClickHouse has extended the usage of expression alias, it can be work in: recursive alias expression: eg: select a + 1 as b, b + 1 as c filter: eg: select a + 1 as b, b + 1 as c from table where c > 0 Note Currently we do not support clickhouse style alias expression. It can be implemented later. For expression alias, we only handle it at last, in projection stage. But We have to replace the alias of the expression as early as possible to prevent ambiguity later. Eg: select number + 1 as c, sum(number) from numbers(10) group by c having c > 3 order by c limit 10 Firstly, we can scan all the alias expressions from projection ASTs. c ---> (number + 1) Then we replaced the alias into the corresponding expression in having , order by , group by clause. So the query will be: select number + 1 as c, sum(number) from numbers(10) group by (number + 1) having (number + 1) > 3 order by (number + 1) limit 10 At last, when the query is finished, we apply the projection to rename the column (number+1) to c Let's take a look at the explain result of this query: | Limit: 10 Projection: (number + 1) as c:UInt64, sum(number):UInt64 Sort: (number + 1):UInt64 Having: ((number + 1) > 3) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum(number)]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum(number)]] Expression: (number + 1):UInt64, number:UInt64 (Before GroupBy) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] We can see we do not need to care about aliasing until the projection, so it will be very convenient to apply other expressions.","title":"Alias Expression"},{"location":"rfcs/query/2021-05-22-plan-expression/#materialized-expression","text":"Materialized expression processing is that we can rebase the expression as a ExpressionColumn if the same expression is already processed upstream. Eg: select number + 1 as c, sum(number) as d group by c having number + 1 > 3 order by d desc After aliases replacement, we will know that order by is sum(number) , but sum(number) is already processed during the aggregating stage, so we can rebase the order by expression SortExpression { ... } to Column(\"sum(number)\") , this could remove useless calculation of same expressions. So number + 1 in having can also apply to rebase the expression.","title":"Materialized Expression"},{"location":"rfcs/query/2021-05-22-plan-expression/#expression-functions","text":"There are many kinds of expression functions. ScalarFunctions, One-to-one calculation process, the result rows is same as the input rows. eg: select database() AggregateFunctions, Many-to-one calculation process, eg: select sum(number) BinaryFunctions, a special kind of \u00b7ScalarFunctions\u00b7 eg: select 1 + 2 ... For ScalarFunctions, we really don't care about the whole block, we just care about the columns involved by the arguments. sum(number) just care about the Column which named number . And the result is also a column, so we have the virtual method in IFunction is: fn eval ( & self , columns : & [ DataColumnarValue ], _input_rows : usize ) -> Result < DataColumnarValue > ; For AggregateFunctions, we should keep the state in the corresponding function instance to apply the two-level merge, we have the following virtual method in IAggregateFunction : fn accumulate ( & mut self , columns : & [ DataColumnarValue ], _input_rows : usize ) -> Result < () > ; fn accumulate_result ( & self ) -> Result < Vec < DataValue >> ; fn merge ( & mut self , _states : & [ DataValue ]) -> Result < () > ; fn merge_result ( & self ) -> Result < DataValue > ; The process is accumulate (apply data to the function) \u2192 accumulate_result (to get the current state) \u2192 merge (merge current state from other state) ---> merge_result (to get the final result value) ps: We don't store the arguments types and arguments names in functions, we can store them later if we need.","title":"Expression Functions"},{"location":"rfcs/query/2021-05-22-plan-expression/#column","text":"Block is the unit of data passed between streams for pipeline processing, while Column is the unit of data passed between expressions. So in the view of expression(functions, literal, ...), everything is Column , we have DataColumnarValue to represent a column. #[derive(Clone, Debug)] pub enum DataColumnarValue { // Array of values. Array ( DataArrayRef ), // A Single value. Constant ( DataValue , usize ) } DataColumnarValue::Constant is like ConstantColumn in ClickHouse . Note: We don't have ScalarValue , because it can be known as Constant(DataValue, 1) , and there is DataValue struct.","title":"Column"},{"location":"rfcs/query/2021-05-22-plan-expression/#expression-chain-and-expression-executor","text":"Currently, we can collect the inner expression from expressions to build ExpressionChain. This could be done by Depth-first-search visiting. ExpressionFunction: number + (number + 1) will be : [ ExpressionColumn(number), ExpressionColumn(number), ExpressionLiteral(1), ExpressionBinary('+', 'number', '1'), ExpressionBinary('+', 'number', '(number + 1)') ] . We have the ExpressionExecutor the execute the expression chain, during the execution, we don't need to care about the kind of the arguments. We just consider them as ColumnExpression from upstream, so we just fetch the column number and the column (number + 1) from the block.","title":"Expression chain and expression executor"},{"location":"rfcs/query/2021-05-22-plan-expression/#plan-builder","text":"","title":"Plan Builder"},{"location":"rfcs/query/2021-05-22-plan-expression/#none-aggregation-query","text":"This is for queries without group by and aggregate functions . Eg: explain select number + 1 as b from numbers(10) where number + 1 > 3 order by number + 3 | explain | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: (number + 1) as b:UInt64 Sort: (number + 3):UInt64 Expression: (number + 1):UInt64, (number + 3):UInt64 (Before OrderBy) Filter: ((number + 1) > 3) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.02 sec) The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 1) > 3 , the schema keeps the same, schema \u2192 [number] Expression: we will collect expressions from order by and having clauses to apply the expression, schema \u2192 [number, number + 1, number + 3] Sort: since we already have the number + 1 in the input plan, so the sorting will consider number + 1 as ColumnExpression , schema \u2192 [number, number + 1, number + 3] Projection: applying the aliases and projection the columns, schema \u2192 [b]","title":"None aggregation query"},{"location":"rfcs/query/2021-05-22-plan-expression/#aggregation-query","text":"To build Aggregation query, there will be more complex than the previous one. Eg: explain select number + 1 as b, sum(number + 2 ) + 4 as c from numbers(10) where number + 3 > 0 group by number + 1 having c > 3 and sum(number + 4) + 1 > 4 order by sum(number + 5) + 1; | Projection: (number + 1) as b:UInt64, (sum((number + 2)) + 4) as c:UInt64 Sort: sum((number + 5)):UInt64 Having: (((sum((number + 2)) + 4) > 3) AND (sum((number + 4)) > 0)) Expression: (number + 1):UInt64, (sum((number + 2)) + 4):UInt64, sum((number + 5)):UInt64 (Before OrderBy) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] Expression: (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Filter: ((number + 3) > 0) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 3) > 0 , the schema keeps the same, schema \u2192 [number] Expression: Before group by (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Before GroupBy, We must visit all the expression in projections , having , group by to collect the expressions and aggregate functions, schema \u2192 [number, number + 1, number + 2, number + 4, number + 5] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] , note that: the expressions are already materialized in upstream, so we just conside all the arguments as columns. AggregatorFinal, schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4))] Expression: schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4)), sum((number + 2)) + 4, sum((number + 5)) + 1] Sort: the schema keeps the same Projection: schema \u2192 b, c","title":"Aggregation query"},{"location":"rfcs/query/2021-05-27-data-shuffle/","text":"Distributed query and data shuffle \u00b6 Summary \u00b6 Distributed query is distributed database necessary feature. This doc is intended to explain the distributed query and its data flow design. Local query \u00b6 Let's see how normal queries run on a single database node. ' +------+ +------------+ +---------+ ' | | AST | | Plan | | ' SQL--->|Parser+------>|Plan Builder+----->|Optimizer| ' | | | | | | ' +------+ +------------+ +---+-----+ ' | Plan ' v ' +----------+ +-----------+ ' | | Processor | | ' Data <------+DataStream|<-----------+Interpreter| ' | | | | ' +----------+ +-----------+ Parser and AST \u00b6 DataFuse uses the third-party SQL parser and its AST. For more information, see: https://github.com/ballista-compute/sqlparser-rs PlanBuilder and Plan \u00b6 A query plan (or query execution plan) is a sequence of steps used to access data in DataFuse. It is built by PlanBuilder from AST. We also use tree to describe it(similar to AST). But it has some differences with AST: Plan is serializable and deserializable. Plan is grammatically safe, we don't worry about it. Plan is used to describe the computation and data dependency, not related to syntax priority We can show it with EXPLAIN SELECT ... mysql> EXPLAIN SELECT number % 3 AS key, SUM(number) AS value FROM numbers(1000) WHERE number > 10 AND number < 990 GROUP BY key ORDER BY key ASC LIMIT 10; +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Limit: 10 Projection: (number % 3) as key:UInt64, SUM(number) as value:UInt64 Sort: (number % 3):UInt64 AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] Expression: (number % 3):UInt64, number:UInt64 (Before GroupBy) Filter: ((number > 10) AND (number < 990)) ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000, read_bytes: 8000] | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Optimizer and Plan \u00b6 For a query, especially a complex query, you can used different plan combinations, orders and structures to get the data . Each of the different ways will get different processing time. So we need to find a reasonable plan combination way in the shortest time, which is what the optimizer does. Interpreter and Processor \u00b6 The interpreter constructs the optimized plan into an executable data stream. We pull the result of SQL by pulling the data in the stream. The calculation logic of each operator in SQL corresponds to a processor, such as FilterPlan -> FilterProcessor, ProjectionPlan -> ProjectionProcessor Distributed query \u00b6 In the cluster mode, we may have to process with some problems different from the standalone mode. In distributed mode, the tables to be queried are always distributed in different nodes For some scenarios, distributed processing is always efficient, such as GROUP BY with keys, JOIN For some scenarios, we have no way of distributed processing, such as LIMIT, GROUP BY without keys In order to ensure fast calculation, we need to coordinate the location of calculation and data. Let's see how normal queries run on a database cluster. ' +------+ +------------+ +------------------+ ' | | AST | | Plan | Optimizer | ' SQL--->|Parser+------>|Plan Builder+----->| | ' | | | | | ScatterOptimizer | ' +------+ +------------+ +--------+---------+ ' | ' +--------------+ | ' | | | ' +--+ FlightStream | <------+ | Plan ' | | | | | ' | +--------------+ | | ' | | | ' | | | ' | | Flight RPC v ' +----------+ Processor | +--------------+ | +----------------+ ' | | RemoteProcessor | | | | do_action | Interpreter | ' Data<--+DataStream|<----------------+--+ FlightStream | <------+------------------+ | ' | | | | | | | PlanRescheduler| ' +----------+ | +--------------+ | +----------------+ ' | | ' | | ' | | ' | | ' | +--------------+ | ' | | | | ' +--+ FlightStream | <------+ ' | | ' +--------------+ ScatterOptimizer and StagePlan \u00b6 In Datafuse, we use ScatterOptimizer to decide the distributed computing of query. In other words, distributed query is an optimization of standalone query. In ScatterOptimizer, we traverse all the plans of the query and rewrite the plan of interest(rewrite as StagePlan { kind:StageKind, input:Self }), where input is the rewritten plan, and kind is an enumeration(Normal: data is shuffled again, Expansive: data spreads from one node to multiple nodes, Convergent: data aggregation from multiple nodes to one node) PlanScheduler and RemoteProcessor \u00b6 In cluster mode, we extract all the StagePlans in the plan optimized by ScatterOptimizer and send them to the corresponding nodes in the cluster according to the kind. For example: mysql> EXPLAIN SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_local(1000000000) GROUP BY user); +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: argMin(user, salary):UInt64 <-- execute in local node AggregatorFinal: groupBy=[[]], aggr=[[argMin(user, salary)]] RedistributeStage[expr: 0] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[]], aggr=[[argMin(user, salary)]] Projection: sum(number) as salary:UInt64, (number % 3) as user:UInt64 AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum(number)]] RedistributeStage[expr: sipHash(_group_by_key)] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum(number)]] Expression: (number % 3):UInt64, number:UInt64 (Before GroupBy) RedistributeStage[expr: blockNumber()] <-- execute in local node ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000000000, read_bytes: 8000000000] | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Flight API DataStream \u00b6 We need to fetch the results of the plans sent to other nodes for execution in some way. FuseData uses the third-party library arrow-flight. more information:[ https://github.com/apache/arrow-rs/tree/master/arrow-flight ]","title":"Distributed query and data shuffle"},{"location":"rfcs/query/2021-05-27-data-shuffle/#distributed-query-and-data-shuffle","text":"","title":"Distributed query and data shuffle"},{"location":"rfcs/query/2021-05-27-data-shuffle/#summary","text":"Distributed query is distributed database necessary feature. This doc is intended to explain the distributed query and its data flow design.","title":"Summary"},{"location":"rfcs/query/2021-05-27-data-shuffle/#local-query","text":"Let's see how normal queries run on a single database node. ' +------+ +------------+ +---------+ ' | | AST | | Plan | | ' SQL--->|Parser+------>|Plan Builder+----->|Optimizer| ' | | | | | | ' +------+ +------------+ +---+-----+ ' | Plan ' v ' +----------+ +-----------+ ' | | Processor | | ' Data <------+DataStream|<-----------+Interpreter| ' | | | | ' +----------+ +-----------+","title":"Local query"},{"location":"rfcs/query/2021-05-27-data-shuffle/#parser-and-ast","text":"DataFuse uses the third-party SQL parser and its AST. For more information, see: https://github.com/ballista-compute/sqlparser-rs","title":"Parser and AST"},{"location":"rfcs/query/2021-05-27-data-shuffle/#planbuilder-and-plan","text":"A query plan (or query execution plan) is a sequence of steps used to access data in DataFuse. It is built by PlanBuilder from AST. We also use tree to describe it(similar to AST). But it has some differences with AST: Plan is serializable and deserializable. Plan is grammatically safe, we don't worry about it. Plan is used to describe the computation and data dependency, not related to syntax priority We can show it with EXPLAIN SELECT ... mysql> EXPLAIN SELECT number % 3 AS key, SUM(number) AS value FROM numbers(1000) WHERE number > 10 AND number < 990 GROUP BY key ORDER BY key ASC LIMIT 10; +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Limit: 10 Projection: (number % 3) as key:UInt64, SUM(number) as value:UInt64 Sort: (number % 3):UInt64 AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] Expression: (number % 3):UInt64, number:UInt64 (Before GroupBy) Filter: ((number > 10) AND (number < 990)) ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000, read_bytes: 8000] | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"PlanBuilder and Plan"},{"location":"rfcs/query/2021-05-27-data-shuffle/#optimizer-and-plan","text":"For a query, especially a complex query, you can used different plan combinations, orders and structures to get the data . Each of the different ways will get different processing time. So we need to find a reasonable plan combination way in the shortest time, which is what the optimizer does.","title":"Optimizer and Plan"},{"location":"rfcs/query/2021-05-27-data-shuffle/#interpreter-and-processor","text":"The interpreter constructs the optimized plan into an executable data stream. We pull the result of SQL by pulling the data in the stream. The calculation logic of each operator in SQL corresponds to a processor, such as FilterPlan -> FilterProcessor, ProjectionPlan -> ProjectionProcessor","title":"Interpreter and Processor"},{"location":"rfcs/query/2021-05-27-data-shuffle/#distributed-query","text":"In the cluster mode, we may have to process with some problems different from the standalone mode. In distributed mode, the tables to be queried are always distributed in different nodes For some scenarios, distributed processing is always efficient, such as GROUP BY with keys, JOIN For some scenarios, we have no way of distributed processing, such as LIMIT, GROUP BY without keys In order to ensure fast calculation, we need to coordinate the location of calculation and data. Let's see how normal queries run on a database cluster. ' +------+ +------------+ +------------------+ ' | | AST | | Plan | Optimizer | ' SQL--->|Parser+------>|Plan Builder+----->| | ' | | | | | ScatterOptimizer | ' +------+ +------------+ +--------+---------+ ' | ' +--------------+ | ' | | | ' +--+ FlightStream | <------+ | Plan ' | | | | | ' | +--------------+ | | ' | | | ' | | | ' | | Flight RPC v ' +----------+ Processor | +--------------+ | +----------------+ ' | | RemoteProcessor | | | | do_action | Interpreter | ' Data<--+DataStream|<----------------+--+ FlightStream | <------+------------------+ | ' | | | | | | | PlanRescheduler| ' +----------+ | +--------------+ | +----------------+ ' | | ' | | ' | | ' | | ' | +--------------+ | ' | | | | ' +--+ FlightStream | <------+ ' | | ' +--------------+","title":"Distributed query"},{"location":"rfcs/query/2021-05-27-data-shuffle/#scatteroptimizer-and-stageplan","text":"In Datafuse, we use ScatterOptimizer to decide the distributed computing of query. In other words, distributed query is an optimization of standalone query. In ScatterOptimizer, we traverse all the plans of the query and rewrite the plan of interest(rewrite as StagePlan { kind:StageKind, input:Self }), where input is the rewritten plan, and kind is an enumeration(Normal: data is shuffled again, Expansive: data spreads from one node to multiple nodes, Convergent: data aggregation from multiple nodes to one node)","title":"ScatterOptimizer and StagePlan"},{"location":"rfcs/query/2021-05-27-data-shuffle/#planscheduler-and-remoteprocessor","text":"In cluster mode, we extract all the StagePlans in the plan optimized by ScatterOptimizer and send them to the corresponding nodes in the cluster according to the kind. For example: mysql> EXPLAIN SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_local(1000000000) GROUP BY user); +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: argMin(user, salary):UInt64 <-- execute in local node AggregatorFinal: groupBy=[[]], aggr=[[argMin(user, salary)]] RedistributeStage[expr: 0] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[]], aggr=[[argMin(user, salary)]] Projection: sum(number) as salary:UInt64, (number % 3) as user:UInt64 AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum(number)]] RedistributeStage[expr: sipHash(_group_by_key)] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum(number)]] Expression: (number % 3):UInt64, number:UInt64 (Before GroupBy) RedistributeStage[expr: blockNumber()] <-- execute in local node ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000000000, read_bytes: 8000000000] | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"PlanScheduler and RemoteProcessor"},{"location":"rfcs/query/2021-05-27-data-shuffle/#flight-api-datastream","text":"We need to fetch the results of the plans sent to other nodes for execution in some way. FuseData uses the third-party library arrow-flight. more information:[ https://github.com/apache/arrow-rs/tree/master/arrow-flight ]","title":"Flight API DataStream"},{"location":"rfcs/store/2021-04-22-store-design/","text":"FuseStore is the storage layer in charge of: - meta data storage such as user, db, table and schema. - blocks life cycle management such as allocation, compaction etc. - data/metadata consistency and reliability. FuseQuery(client) | | rpc v FuseStore | flightServer // network, auth | | | v | Handler // execution engine | | | v | IFileSystem // abstract storage layer IFileSystem \u00b6 IFileSystem defines an abstract storage layer that FuseStore would runs on. An IFileSystem impl in the cluster is the only stateful component. Local FS: impl IFileSystem API and use a local disk folder as storage. Suitable for a single node FuseQuery deployment. DFS: impl IFileSystem and setup an aws-S3 like storage service. A DFS organizes multiple LocalFS with a centralized meta data service. Object Storage Adapters: an IFileSystem impl that builds upon an object storage service on cloud. IFileSystem defines following API-s: add : AKA put-if-absent: add a file only if it is absent. read_all : read all bytes of a file. list : retrieve a list of files with specified prefix. TODO API \u00b6 FuseQuery and FuseStore talks arrow-flight protocol. Schema related operations such as create table or create database are wrapped with a FlightService::do_action RPC. Data operation such as reading or writing a block are done with FlightService::do_get and FlightService::do_put . See common/flights/src/store_client.rs . DFS \u00b6 The most important component in FuseStore is the DFS. DFS mainly consists of two parts: the meta data cluster and block storage cluster. Block cluster is unaware of data placement and is purely a set of servers providing object like write and read API. Data placement is part of the meta data and is stored in the meta cluster. Meta data cluster \u00b6 All meta(the cluster meta and file keys) has a copy that resides in memory on every node for instant access. Every update to meta is done by committing a raft log of the updated entry into meta cluster. A cluster keeps its meta data in a raft group with typically 5 candidate nodes and all other nodes as learners. Candidate nodes provides meta data read and write API. Every other node is a learner , which does not elect but just subscribes meta change message from the 5 candidates. In-process metadata components \u00b6 A FuseStore process includes two grpc API: the flight service and the meta service. Meta related components are wrapped into MetaNode , in which a Raft instance is maintained along with storage and netowrk engines. MetaNode is the only entry for other FuseStore components to access meta data. RaftNode communicates with remote RaftNode through Network , which send messages to meta-grpc service on other FuseStore nodes. Network relies on Storage to find out info of other nodes. FuseStore components: .---------------------------. | | | flight-grpc meta-grpc | | | | | | '--. .-----' | | v v | | MetaNode | | | | | | | v | | | RaftNode | | | .--' | | | v v v | | Storage <- Network | | | '---------------------------' Meta data structure \u00b6 Meta data includes hardware information: nodes, the file information: keys and data placement information: slots. message Meta { map < string , string > keys ; repeated Slot slots ; map < int64 , Node > nodes } message Node { int64 NodeId repeated string Addresses } message Slot { repeated int64 node_ids ; } File format \u00b6 A data block in DFS or local-FS is a complete Parquet data, with schema embedded. The schema in block should always be identical to the schema stored in table-meta. Otherwise it is a severe bug. Parquet has its own segmentation and index in it which is similar to ClickHouse file structure. See: https://parquet.apache.org/documentation/latest/ A schema file such as table or database is in protobuf format. Data placement \u00b6 A file in DFS has 3 copies. A file is identified with a unique key . Every key is assigned to a virtual allocation unit slot by some hash algo. A slot is assigned to 3 nodes. The slot-to-nodes mapping is part of the DFS meta data. Replication \u00b6 Once a data block is persisted on local fs and the corresponding meta data is committed, DFS ack the client an OK message. Every node is either a follower or leaner of the meta data raft group thus will receive the meta changes. If a node found that a key is uploaded and the slot for the key is on this node, it pulls the data block from the uploading node. Meta cluster startup \u00b6 A FuseStore cluster is stateful thus the startup is done in several steps: Boot up the first node in a cluster, by calling MetaNode::boot() . This func creates an empty raft instance add initialize itself as the leader of the solo cluster. Creating node is done with MetaNode::boot_non_voter() . This func does nothing more than initializing a raft instance. It returns the MetaNode instance with network API ready for accepting raft communications. At this point it is unaware of anything about the cluster. To add this new node to a cluster, send an Cmd::AddNode request to the leader. The leader will then commit the node info into its own storage and start sending raft snapshot and logs to the new node. When a node is shutting down and restarted, just the MetaNode::new() is used to start the meta-service. When a node is restarted: A candidate(AKA voter) that becomes the new leader is able to find out every node from its local storage and then add them as non-voter in order to replicate logs to them. A non-voter has nothing to do other than receiving logs from the leader. DFS Example \u00b6 FQ: FuseQuery node flight: arrow-flight server handler: execution handler DFS: distributed FS FS: local FS L: leader of raft for meta F: follower of raft lnr: learner FQ | | 1. put block | or create table | -----------------|------------------------------------- v flight <-----. flight | | | 2. | 8. pull block 5. commit | | meta v | .-------handler '- handler | | | ^ | | 3. 9. | | | v v | | DFS DFS| | | | | | | 4. 10. | | | v v | | FS FS | 7. notify handler | | of meta changes v 6. meta bcast | meta : L----+-------+------+------|-+-------. `->F `->F `->F | `->F `->lnr `-' nodes: N1 N2 N3 N4 N5 N6 ... Table format \u00b6 A table in IFileSystem consists of several files and the structure is similar to a append-only log: Table head: contains schema and a pointer to the latest manifest file. The table head must be updated atomically. Manifest: describes what data blocks belongs to a table. There is a list of data block files pointing to the latest updates, and a pointer to previous manifest i.e., the last version this update based on. Data: a collection of operation logs. T: table head Mi: manifest di: data block files T | v M0 <---- M1 <----- M2 | | | `-> d0 +-> d1 +-> d3 | | `-> d2 `-> d4 A typical update workflow would be like the following: Handler receives a batch of updates, including inserts and deletes. It writes to one or more data blocks, with some unique keys. Handler reads the latest table head file, finds out the latest manifest, e.g., M1, and compose a new manifest e.g., M2, containing the data block keys from step-1 and a pointer to M1. Write it to DFS with a unique key. Handler atomically update the table head file to change the latest manifest pointer to M2. If race encountered, retry from step-2. The challenge is on step-3: An IFileSystem impl may not support atomic update. In this case, write the table head file with a mono-incremental key, e.g. t-001 , then t-002 ... And a read operation should list all of the heads and find out the latest version(resulting in eventual consistency). This strategy only requires atomic-add operation, i.e., put-if-absent. Atomic-add can be done on behalf of the meta cluster, since it is an easy job for a raft group or any other consensus group to generate mono-incremental ids. Table compaction \u00b6 Compaction merges several earliest manifest into one, and optionally merges overlapping data blocks. Compaction generates a new manifest e.g., M1' from M1. Then it removes M0 and M1. A reading process should try to read both M1 and M1', and use either one it sees. A manifest or data block will be added or removed, but never updated, since in a distributed system updating data results in super complicated consistency challenges . T | v M0 <---- M1 <----+ M2 | | | | `-> d0 +-> d1 | +-> d3 | | | `-> d2 | `-> d4 | | | | M1' <---' +-> d0 | +-> d1 | `-> d2","title":"FuseStore Design"},{"location":"rfcs/store/2021-04-22-store-design/#ifilesystem","text":"IFileSystem defines an abstract storage layer that FuseStore would runs on. An IFileSystem impl in the cluster is the only stateful component. Local FS: impl IFileSystem API and use a local disk folder as storage. Suitable for a single node FuseQuery deployment. DFS: impl IFileSystem and setup an aws-S3 like storage service. A DFS organizes multiple LocalFS with a centralized meta data service. Object Storage Adapters: an IFileSystem impl that builds upon an object storage service on cloud. IFileSystem defines following API-s: add : AKA put-if-absent: add a file only if it is absent. read_all : read all bytes of a file. list : retrieve a list of files with specified prefix. TODO","title":"IFileSystem"},{"location":"rfcs/store/2021-04-22-store-design/#api","text":"FuseQuery and FuseStore talks arrow-flight protocol. Schema related operations such as create table or create database are wrapped with a FlightService::do_action RPC. Data operation such as reading or writing a block are done with FlightService::do_get and FlightService::do_put . See common/flights/src/store_client.rs .","title":"API"},{"location":"rfcs/store/2021-04-22-store-design/#dfs","text":"The most important component in FuseStore is the DFS. DFS mainly consists of two parts: the meta data cluster and block storage cluster. Block cluster is unaware of data placement and is purely a set of servers providing object like write and read API. Data placement is part of the meta data and is stored in the meta cluster.","title":"DFS"},{"location":"rfcs/store/2021-04-22-store-design/#meta-data-cluster","text":"All meta(the cluster meta and file keys) has a copy that resides in memory on every node for instant access. Every update to meta is done by committing a raft log of the updated entry into meta cluster. A cluster keeps its meta data in a raft group with typically 5 candidate nodes and all other nodes as learners. Candidate nodes provides meta data read and write API. Every other node is a learner , which does not elect but just subscribes meta change message from the 5 candidates.","title":"Meta data cluster"},{"location":"rfcs/store/2021-04-22-store-design/#in-process-metadata-components","text":"A FuseStore process includes two grpc API: the flight service and the meta service. Meta related components are wrapped into MetaNode , in which a Raft instance is maintained along with storage and netowrk engines. MetaNode is the only entry for other FuseStore components to access meta data. RaftNode communicates with remote RaftNode through Network , which send messages to meta-grpc service on other FuseStore nodes. Network relies on Storage to find out info of other nodes. FuseStore components: .---------------------------. | | | flight-grpc meta-grpc | | | | | | '--. .-----' | | v v | | MetaNode | | | | | | | v | | | RaftNode | | | .--' | | | v v v | | Storage <- Network | | | '---------------------------'","title":"In-process metadata components"},{"location":"rfcs/store/2021-04-22-store-design/#meta-data-structure","text":"Meta data includes hardware information: nodes, the file information: keys and data placement information: slots. message Meta { map < string , string > keys ; repeated Slot slots ; map < int64 , Node > nodes } message Node { int64 NodeId repeated string Addresses } message Slot { repeated int64 node_ids ; }","title":"Meta data structure"},{"location":"rfcs/store/2021-04-22-store-design/#file-format","text":"A data block in DFS or local-FS is a complete Parquet data, with schema embedded. The schema in block should always be identical to the schema stored in table-meta. Otherwise it is a severe bug. Parquet has its own segmentation and index in it which is similar to ClickHouse file structure. See: https://parquet.apache.org/documentation/latest/ A schema file such as table or database is in protobuf format.","title":"File format"},{"location":"rfcs/store/2021-04-22-store-design/#data-placement","text":"A file in DFS has 3 copies. A file is identified with a unique key . Every key is assigned to a virtual allocation unit slot by some hash algo. A slot is assigned to 3 nodes. The slot-to-nodes mapping is part of the DFS meta data.","title":"Data placement"},{"location":"rfcs/store/2021-04-22-store-design/#replication","text":"Once a data block is persisted on local fs and the corresponding meta data is committed, DFS ack the client an OK message. Every node is either a follower or leaner of the meta data raft group thus will receive the meta changes. If a node found that a key is uploaded and the slot for the key is on this node, it pulls the data block from the uploading node.","title":"Replication"},{"location":"rfcs/store/2021-04-22-store-design/#meta-cluster-startup","text":"A FuseStore cluster is stateful thus the startup is done in several steps: Boot up the first node in a cluster, by calling MetaNode::boot() . This func creates an empty raft instance add initialize itself as the leader of the solo cluster. Creating node is done with MetaNode::boot_non_voter() . This func does nothing more than initializing a raft instance. It returns the MetaNode instance with network API ready for accepting raft communications. At this point it is unaware of anything about the cluster. To add this new node to a cluster, send an Cmd::AddNode request to the leader. The leader will then commit the node info into its own storage and start sending raft snapshot and logs to the new node. When a node is shutting down and restarted, just the MetaNode::new() is used to start the meta-service. When a node is restarted: A candidate(AKA voter) that becomes the new leader is able to find out every node from its local storage and then add them as non-voter in order to replicate logs to them. A non-voter has nothing to do other than receiving logs from the leader.","title":"Meta cluster startup"},{"location":"rfcs/store/2021-04-22-store-design/#dfs-example","text":"FQ: FuseQuery node flight: arrow-flight server handler: execution handler DFS: distributed FS FS: local FS L: leader of raft for meta F: follower of raft lnr: learner FQ | | 1. put block | or create table | -----------------|------------------------------------- v flight <-----. flight | | | 2. | 8. pull block 5. commit | | meta v | .-------handler '- handler | | | ^ | | 3. 9. | | | v v | | DFS DFS| | | | | | | 4. 10. | | | v v | | FS FS | 7. notify handler | | of meta changes v 6. meta bcast | meta : L----+-------+------+------|-+-------. `->F `->F `->F | `->F `->lnr `-' nodes: N1 N2 N3 N4 N5 N6 ...","title":"DFS Example"},{"location":"rfcs/store/2021-04-22-store-design/#table-format","text":"A table in IFileSystem consists of several files and the structure is similar to a append-only log: Table head: contains schema and a pointer to the latest manifest file. The table head must be updated atomically. Manifest: describes what data blocks belongs to a table. There is a list of data block files pointing to the latest updates, and a pointer to previous manifest i.e., the last version this update based on. Data: a collection of operation logs. T: table head Mi: manifest di: data block files T | v M0 <---- M1 <----- M2 | | | `-> d0 +-> d1 +-> d3 | | `-> d2 `-> d4 A typical update workflow would be like the following: Handler receives a batch of updates, including inserts and deletes. It writes to one or more data blocks, with some unique keys. Handler reads the latest table head file, finds out the latest manifest, e.g., M1, and compose a new manifest e.g., M2, containing the data block keys from step-1 and a pointer to M1. Write it to DFS with a unique key. Handler atomically update the table head file to change the latest manifest pointer to M2. If race encountered, retry from step-2. The challenge is on step-3: An IFileSystem impl may not support atomic update. In this case, write the table head file with a mono-incremental key, e.g. t-001 , then t-002 ... And a read operation should list all of the heads and find out the latest version(resulting in eventual consistency). This strategy only requires atomic-add operation, i.e., put-if-absent. Atomic-add can be done on behalf of the meta cluster, since it is an easy job for a raft group or any other consensus group to generate mono-incremental ids.","title":"Table format"},{"location":"rfcs/store/2021-04-22-store-design/#table-compaction","text":"Compaction merges several earliest manifest into one, and optionally merges overlapping data blocks. Compaction generates a new manifest e.g., M1' from M1. Then it removes M0 and M1. A reading process should try to read both M1 and M1', and use either one it sees. A manifest or data block will be added or removed, but never updated, since in a distributed system updating data results in super complicated consistency challenges . T | v M0 <---- M1 <----+ M2 | | | | `-> d0 +-> d1 | +-> d3 | | | `-> d2 | `-> d4 | | | | M1' <---' +-> d0 | +-> d1 | `-> d2","title":"Table compaction"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/","text":"Calculates the arg value for a maximum val value. If there are several different values of arg for maximum values of val , returns the first of these values encountered. Syntax \u00b6 argMax(arg, val) Arguments \u00b6 Arguments Description arg Argument val Value Return Type \u00b6 arg value that corresponds to maximum val value. matches arg type. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMax(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMax(user, salary) | +----------------------+ | 0 | +----------------------+","title":"argMax"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#syntax","text":"argMax(arg, val)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#arguments","text":"Arguments Description arg Argument val Value","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#return-type","text":"arg value that corresponds to maximum val value. matches arg type.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMax(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMax(user, salary) | +----------------------+ | 0 | +----------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/","text":"Calculates the arg value for a minimum val value. If there are several different values of arg for minimum values of val , returns the first of these values encountered. Syntax \u00b6 argMin(arg, val) Arguments \u00b6 Arguments Description arg Argument val Value Return Type \u00b6 arg value that corresponds to minimum val value. matches arg type. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMin(user, salary) | +----------------------+ | 1 | +----------------------+","title":"argMin"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#syntax","text":"argMin(arg, val)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#arguments","text":"Arguments Description arg Argument val Value","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#return-type","text":"arg value that corresponds to minimum val value. matches arg type.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMin(user, salary) | +----------------------+ | 1 | +----------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/","text":"Aggregate function. The AVG() function returns the average value of an expression. Note: NULL values are not counted. Syntax \u00b6 AVG ( expression ) Arguments \u00b6 Arguments Description expression Any numerical expression Return Type \u00b6 double Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT AVG(*) FROM numbers(3); +--------+ | avg(*) | +--------+ | 1 | +--------+ mysql> SELECT AVG(number) FROM numbers(3); +-------------+ | avg(number) | +-------------+ | 1 | +-------------+ mysql> SELECT AVG(number+1) FROM numbers(3); +----------------------+ | avg(plus(number, 1)) | +----------------------+ | 2 | +----------------------+ mysql> SELECT AVG(number+1) AS a FROM numbers(3); +------+ | a | +------+ | 2 | +------+","title":"AVG"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#syntax","text":"AVG ( expression )","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#arguments","text":"Arguments Description expression Any numerical expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#return-type","text":"double","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT AVG(*) FROM numbers(3); +--------+ | avg(*) | +--------+ | 1 | +--------+ mysql> SELECT AVG(number) FROM numbers(3); +-------------+ | avg(number) | +-------------+ | 1 | +-------------+ mysql> SELECT AVG(number+1) FROM numbers(3); +----------------------+ | avg(plus(number, 1)) | +----------------------+ | 2 | +----------------------+ mysql> SELECT AVG(number+1) AS a FROM numbers(3); +------+ | a | +------+ | 2 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-count/","text":"Aggregate function. The COUNT() function returns the number of records returned by a select query. Note: NULL values are not counted. Syntax \u00b6 COUNT(expression) Arguments \u00b6 Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. * is also allowed, to indicate pure row counting. Return Type \u00b6 An integer. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT count(*) FROM numbers(3); +----------+ | count(*) | +----------+ | 3 | +----------+ mysql> SELECT count(number) FROM numbers(3); +---------------+ | count(number) | +---------------+ | 3 | +---------------+ mysql> SELECT count(number) AS c FROM numbers(3); +------+ | c | +------+ | 3 | +------+","title":"COUNT"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#syntax","text":"COUNT(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#arguments","text":"Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. * is also allowed, to indicate pure row counting.","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#return-type","text":"An integer.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT count(*) FROM numbers(3); +----------+ | count(*) | +----------+ | 3 | +----------+ mysql> SELECT count(number) FROM numbers(3); +---------------+ | count(number) | +---------------+ | 3 | +---------------+ mysql> SELECT count(number) AS c FROM numbers(3); +------+ | c | +------+ | 3 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-max/","text":"Aggregate function. The MAX() function returns the maximum value in a set of values. Syntax \u00b6 MAX(expression) Arguments \u00b6 Arguments Description expression Any expression Return Type \u00b6 The maximum value, in the type of the value. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MAX(*) FROM numbers(3); +--------+ | max(*) | +--------+ | 2 | +--------+ mysql> SELECT MAX(number) FROM numbers(3); +-------------+ | max(number) | +-------------+ | 2 | +-------------+ mysql> SELECT MAX(number) AS max FROM numbers(3); +------+ | max | +------+ | 2 | +------+","title":"MAX"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#syntax","text":"MAX(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#arguments","text":"Arguments Description expression Any expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#return-type","text":"The maximum value, in the type of the value.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MAX(*) FROM numbers(3); +--------+ | max(*) | +--------+ | 2 | +--------+ mysql> SELECT MAX(number) FROM numbers(3); +-------------+ | max(number) | +-------------+ | 2 | +-------------+ mysql> SELECT MAX(number) AS max FROM numbers(3); +------+ | max | +------+ | 2 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-min/","text":"Aggregate function. The MIN() function returns the minimum value in a set of values. Syntax \u00b6 MIN(expression) Arguments \u00b6 Arguments Description expression Any expression Return Type \u00b6 The minimum value, in the type of the value. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MIN(*) FROM numbers(3); +--------+ | min(*) | +--------+ | 0 | +--------+ mysql> SELECT MIN(number) FROM numbers(3); +-------------+ | min(number) | +-------------+ | 0 | +-------------+ mysql> SELECT MIN(number) AS min FROM numbers(3); +------+ | min | +------+ | 0 | +------+","title":"MIN"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#syntax","text":"MIN(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#arguments","text":"Arguments Description expression Any expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#return-type","text":"The minimum value, in the type of the value.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MIN(*) FROM numbers(3); +--------+ | min(*) | +--------+ | 0 | +--------+ mysql> SELECT MIN(number) FROM numbers(3); +-------------+ | min(number) | +-------------+ | 0 | +-------------+ mysql> SELECT MIN(number) AS min FROM numbers(3); +------+ | min | +------+ | 0 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/","text":"Aggregate function. The SUM() function calculates the sum of a set of values. Note: NULL values are not counted. Syntax \u00b6 SUM(expression) Arguments \u00b6 Arguments Description expression Any expression Return Type \u00b6 A double if the input type is double, otherwise integer. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT SUM(*) FROM numbers(3); +--------+ | sum(*) | +--------+ | 3 | +--------+ mysql> SELECT SUM(number) FROM numbers(3); +-------------+ | sum(number) | +-------------+ | 3 | +-------------+ mysql> SELECT SUM(number) AS sum FROM numbers(3); +------+ | sum | +------+ | 3 | +------+ mysql> SELECT SUM(number+2) AS sum FROM numbers(3); +------+ | sum | +------+ | 9 | +------+","title":"SUM"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#syntax","text":"SUM(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#arguments","text":"Arguments Description expression Any expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#return-type","text":"A double if the input type is double, otherwise integer.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT SUM(*) FROM numbers(3); +--------+ | sum(*) | +--------+ | 3 | +--------+ mysql> SELECT SUM(number) FROM numbers(3); +-------------+ | sum(number) | +-------------+ | 3 | +-------------+ mysql> SELECT SUM(number) AS sum FROM numbers(3); +------+ | sum | +------+ | 3 | +------+ mysql> SELECT SUM(number+2) AS sum FROM numbers(3); +------+ | sum | +------+ | 9 | +------+","title":"Examples"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-database/","text":"Create a database. Syntax \u00b6 CREATE DATABASE < database_name > Examples \u00b6 mysql > CREATE DATABASE test ;","title":"CREATE DATABASE"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-database/#syntax","text":"CREATE DATABASE < database_name >","title":"Syntax"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-database/#examples","text":"mysql > CREATE DATABASE test ;","title":"Examples"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-database/","text":"Drop a database. Syntax \u00b6 DROP DATABASE [ IF EXISTS ] < database_name > Examples \u00b6 mysql > DROP DATABASE test ;","title":"DROP DATABASE"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-database/#syntax","text":"DROP DATABASE [ IF EXISTS ] < database_name >","title":"Syntax"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-database/#examples","text":"mysql > DROP DATABASE test ;","title":"Examples"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/","text":"Retrieves data from a table. Syntax \u00b6 SELECT [ALL | DISTINCT] select_expr [[AS] alias], ... [INTO variable [, ...]] [ FROM table_references [WHERE expr] [GROUP BY {{col_name | expr | position}, ... | extended_grouping_expr}] [HAVING expr] [ORDER BY {col_name | expr} [ASC | DESC], ...] [LIMIT row_count] ] Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. SELECT clause \u00b6 mysql> SELECT number FROM numbers(3); +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+ FROM clause \u00b6 mysql> SELECT number FROM numbers(3) AS a; +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+ WHERE clause \u00b6 mysql> SELECT number FROM numbers(3) WHERE number > 1; +--------+ | number | +--------+ | 2 | +--------+ 1 row in set (0.00 sec) GROUP BY clause \u00b6 mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) FROM numbers(10000) GROUP BY c1, c2; +------+------+-------------+ | c1 | c2 | MAX(number) | +------+------+-------------+ | 1 | 2 | 9995 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | | 0 | 1 | 9994 | | 0 | 0 | 9996 | | 1 | 0 | 9999 | +------+------+-------------+ 6 rows in set (0.00 sec) HAVING clause \u00b6 mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) as max FROM numbers(10000) GROUP BY c1, c2 HAVING max>9996; +------+------+------+ | c1 | c2 | max | +------+------+------+ | 1 | 0 | 9999 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | +------+------+------+ 3 rows in set (0.00 sec) ORDER By clause \u00b6 mysql> SELECT number FROM numbers(5) ORDER BY number ASC; +--------+ | number | +--------+ | 0 | | 1 | | 2 | | 3 | | 4 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number FROM numbers(5) ORDER BY number DESC; +--------+ | number | +--------+ | 4 | | 3 | | 2 | | 1 | | 0 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number%2 AS c1, number%3 AS c2 FROM numbers(5) ORDER BY c1 ASC, c2 DESC; +------+------+ | c1 | c2 | +------+------+ | 0 | 2 | | 0 | 1 | | 0 | 0 | | 1 | 1 | | 1 | 0 | +------+------+ 5 rows in set (0.00 sec) LIMIT clause \u00b6 mysql> SELECT number FROM numbers(1000000000) LIMIT 1; +--------+ | number | +--------+ | 0 | +--------+ 1 row in set (0.00 sec) Nested Sub-Selects \u00b6 SELECT statements can be nested in queries. SELECT ... [SELECT ...[SELECT [...]]] mysql> SELECT MIN(number) FROM (SELECT number%3 AS number FROM numbers(10)) GROUP BY number%2; +-------------+ | min(number) | +-------------+ | 1 | | 0 | +-------------+","title":"SELECT"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#syntax","text":"SELECT [ALL | DISTINCT] select_expr [[AS] alias], ... [INTO variable [, ...]] [ FROM table_references [WHERE expr] [GROUP BY {{col_name | expr | position}, ... | extended_grouping_expr}] [HAVING expr] [ORDER BY {col_name | expr} [ASC | DESC], ...] [LIMIT row_count] ] Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1.","title":"Syntax"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#select-clause","text":"mysql> SELECT number FROM numbers(3); +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+","title":"SELECT clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#from-clause","text":"mysql> SELECT number FROM numbers(3) AS a; +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+","title":"FROM clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#where-clause","text":"mysql> SELECT number FROM numbers(3) WHERE number > 1; +--------+ | number | +--------+ | 2 | +--------+ 1 row in set (0.00 sec)","title":"WHERE clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#group-by-clause","text":"mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) FROM numbers(10000) GROUP BY c1, c2; +------+------+-------------+ | c1 | c2 | MAX(number) | +------+------+-------------+ | 1 | 2 | 9995 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | | 0 | 1 | 9994 | | 0 | 0 | 9996 | | 1 | 0 | 9999 | +------+------+-------------+ 6 rows in set (0.00 sec)","title":"GROUP BY clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#having-clause","text":"mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) as max FROM numbers(10000) GROUP BY c1, c2 HAVING max>9996; +------+------+------+ | c1 | c2 | max | +------+------+------+ | 1 | 0 | 9999 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | +------+------+------+ 3 rows in set (0.00 sec)","title":"HAVING clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#order-by-clause","text":"mysql> SELECT number FROM numbers(5) ORDER BY number ASC; +--------+ | number | +--------+ | 0 | | 1 | | 2 | | 3 | | 4 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number FROM numbers(5) ORDER BY number DESC; +--------+ | number | +--------+ | 4 | | 3 | | 2 | | 1 | | 0 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number%2 AS c1, number%3 AS c2 FROM numbers(5) ORDER BY c1 ASC, c2 DESC; +------+------+ | c1 | c2 | +------+------+ | 0 | 2 | | 0 | 1 | | 0 | 0 | | 1 | 1 | | 1 | 0 | +------+------+ 5 rows in set (0.00 sec)","title":"ORDER By clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#limit-clause","text":"mysql> SELECT number FROM numbers(1000000000) LIMIT 1; +--------+ | number | +--------+ | 0 | +--------+ 1 row in set (0.00 sec)","title":"LIMIT clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#nested-sub-selects","text":"SELECT statements can be nested in queries. SELECT ... [SELECT ...[SELECT [...]]] mysql> SELECT MIN(number) FROM (SELECT number%3 AS number FROM numbers(10)) GROUP BY number%2; +-------------+ | min(number) | +-------------+ | 1 | | 0 | +-------------+","title":"Nested Sub-Selects"},{"location":"sqlstatement/hash-functions/siphash/","text":"Produces a 64-bit SipHash hash value. Syntax \u00b6 SIPHASH ( expression ) Arguments \u00b6 Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. Return Type \u00b6 A UInt64 data type hash value. Examples \u00b6 mysql> SELECT SIPHASH('1234567890'); +----------------------+ | SIPHASH(1234567890) | +----------------------+ | 18110648197875983073 | +----------------------+ mysql> SELECT SIPHASH(1); +---------------------+ | SIPHASH(1) | +---------------------+ | 2206609067086327257 | +---------------------+ mysql> SELECT SIPHASH(1.2); +---------------------+ | SIPHASH(1.2) | +---------------------+ | 2854037594257667269 | +---------------------+ mysql> SELECT SIPHASH(number) FROM numbers(2); +----------------------+ | siphash(number) | +----------------------+ | 13646096770106105413 | | 2206609067086327257 | +----------------------+","title":"SIPHASH"},{"location":"sqlstatement/hash-functions/siphash/#syntax","text":"SIPHASH ( expression )","title":"Syntax"},{"location":"sqlstatement/hash-functions/siphash/#arguments","text":"Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation.","title":"Arguments"},{"location":"sqlstatement/hash-functions/siphash/#return-type","text":"A UInt64 data type hash value.","title":"Return Type"},{"location":"sqlstatement/hash-functions/siphash/#examples","text":"mysql> SELECT SIPHASH('1234567890'); +----------------------+ | SIPHASH(1234567890) | +----------------------+ | 18110648197875983073 | +----------------------+ mysql> SELECT SIPHASH(1); +---------------------+ | SIPHASH(1) | +---------------------+ | 2206609067086327257 | +---------------------+ mysql> SELECT SIPHASH(1.2); +---------------------+ | SIPHASH(1.2) | +---------------------+ | 2854037594257667269 | +---------------------+ mysql> SELECT SIPHASH(number) FROM numbers(2); +----------------------+ | siphash(number) | +----------------------+ | 13646096770106105413 | | 2206609067086327257 | +----------------------+","title":"Examples"},{"location":"sqlstatement/information-functions/database/","text":"Returns the name of the currently selected database. If no database is selected, then this function returns default . Syntax \u00b6 SELECT DATABASE() Examples \u00b6 mysql> SELECT DATABASE(); +------------+ | database() | +------------+ | default | +------------+","title":"DATABASE"},{"location":"sqlstatement/information-functions/database/#syntax","text":"SELECT DATABASE()","title":"Syntax"},{"location":"sqlstatement/information-functions/database/#examples","text":"mysql> SELECT DATABASE(); +------------+ | database() | +------------+ | default | +------------+","title":"Examples"},{"location":"sqlstatement/other-functions/totypename/","text":"ToTypeName function is used to return the name of a data type. Syntax \u00b6 ToTypeName ( expression ) Arguments \u00b6 Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. Return Type \u00b6 String Examples \u00b6 mysql> SELECT ToTypeName(number) FROM numbers(2); +--------------------+ | ToTypeName(number) | +--------------------+ | UInt64 | | UInt64 | +--------------------+ mysql> SELECT ToTypeName(sum(number)) FROM numbers(2); +-------------------------+ | ToTypeName(sum(number)) | +-------------------------+ | UInt64 | +-------------------------+","title":"ToTypeName"},{"location":"sqlstatement/other-functions/totypename/#syntax","text":"ToTypeName ( expression )","title":"Syntax"},{"location":"sqlstatement/other-functions/totypename/#arguments","text":"Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation.","title":"Arguments"},{"location":"sqlstatement/other-functions/totypename/#return-type","text":"String","title":"Return Type"},{"location":"sqlstatement/other-functions/totypename/#examples","text":"mysql> SELECT ToTypeName(number) FROM numbers(2); +--------------------+ | ToTypeName(number) | +--------------------+ | UInt64 | | UInt64 | +--------------------+ mysql> SELECT ToTypeName(sum(number)) FROM numbers(2); +-------------------------+ | ToTypeName(sum(number)) | +-------------------------+ | UInt64 | +-------------------------+","title":"Examples"},{"location":"sqlstatement/show-commands/show-databases/","text":"Shows the list of databases that exist on the instance. Syntax \u00b6 SHOW DATABASES Examples \u00b6 mysql> SHOW DATABASES; +----------+ | name | +----------+ | default | | for_test | | local | | system | | test | +----------+","title":"SHOW DATABASES"},{"location":"sqlstatement/show-commands/show-databases/#syntax","text":"SHOW DATABASES","title":"Syntax"},{"location":"sqlstatement/show-commands/show-databases/#examples","text":"mysql> SHOW DATABASES; +----------+ | name | +----------+ | default | | for_test | | local | | system | | test | +----------+","title":"Examples"},{"location":"sqlstatement/show-commands/show-tables/","text":"Shows the list of tables in the currently selected database. Syntax \u00b6 SHOW TABLES Examples \u00b6 mysql> USE system; mysql> SHOW TABLES; +----------+ | name | +----------+ | numbers | +----------+","title":"SHOW TABLES"},{"location":"sqlstatement/show-commands/show-tables/#syntax","text":"SHOW TABLES","title":"Syntax"},{"location":"sqlstatement/show-commands/show-tables/#examples","text":"mysql> USE system; mysql> SHOW TABLES; +----------+ | name | +----------+ | numbers | +----------+","title":"Examples"},{"location":"sqlstatement/string-functions/substring/","text":"SUBSTRING function is used to extract a string containing a specific number of characters from a particular position of a given string. Syntax \u00b6 SUBSTRING ( expression [ FROM position_expr ] [ FOR length_expr ]) Arguments \u00b6 Arguments Description expression The main string from where the character to be extracted position_expr The one-indexed position expression to start at. If negative, counts from the end length_expr The number expression of characters to extract Return Type \u00b6 String Note In SUBSTRING, the starting index point of a string is 1 (not 0). In the following example, the starting index 3 represents the third character in the string, because the index starts from 1. mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+ Examples \u00b6 mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+ mysql> SELECT SUBSTRING('1234567890' FROM 3); +------------------------------+ | SUBSTRING(1234567890,3,NULL) | +------------------------------+ | 34567890 | +------------------------------+ 1 row in set (0.01 sec) mysql> SELECT SUBSTRING('1234567890' FOR 3); +---------------------------+ | SUBSTRING(1234567890,1,3) | +---------------------------+ | 123 | +---------------------------+ 1 row in set (0.01 sec)","title":"SUBSTRING"},{"location":"sqlstatement/string-functions/substring/#syntax","text":"SUBSTRING ( expression [ FROM position_expr ] [ FOR length_expr ])","title":"Syntax"},{"location":"sqlstatement/string-functions/substring/#arguments","text":"Arguments Description expression The main string from where the character to be extracted position_expr The one-indexed position expression to start at. If negative, counts from the end length_expr The number expression of characters to extract","title":"Arguments"},{"location":"sqlstatement/string-functions/substring/#return-type","text":"String Note In SUBSTRING, the starting index point of a string is 1 (not 0). In the following example, the starting index 3 represents the third character in the string, because the index starts from 1. mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+","title":"Return Type"},{"location":"sqlstatement/string-functions/substring/#examples","text":"mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+ mysql> SELECT SUBSTRING('1234567890' FROM 3); +------------------------------+ | SUBSTRING(1234567890,3,NULL) | +------------------------------+ | 34567890 | +------------------------------+ 1 row in set (0.01 sec) mysql> SELECT SUBSTRING('1234567890' FOR 3); +---------------------------+ | SUBSTRING(1234567890,1,3) | +---------------------------+ | 123 | +---------------------------+ 1 row in set (0.01 sec)","title":"Examples"},{"location":"system/system-tables/","text":"Most system tables store their data in RAM. A FuseQuery server creates such system tables at the start. system.numbers \u00b6 This table contains a single UInt64 column named number that contains almost all the natural numbers starting from zero. You can use this table for tests, or if you need to do a brute force search. Reads from this table are parallelized too. Used for tests. mysql> SELECT avg(number) FROM numbers(100000000); +-------------+ | avg(number) | +-------------+ | 49999999.5 | +-------------+ 1 row in set (0.04 sec) system.numbers_mt \u00b6 The same as system.numbers system.settings \u00b6 Contains information about session settings for current user. mysql> SELECT * FROM system.settings; +----------------+---------+---------------------------------------------------------------------------------------------------+ | name | value | description | +----------------+---------+---------------------------------------------------------------------------------------------------+ | max_block_size | 10000 | Maximum block size for reading | | max_threads | 8 | The maximum number of threads to execute the request. By default, it is determined automatically. | | default_db | default | The default database for current session | +----------------+---------+---------------------------------------------------------------------------------------------------+ 3 rows in set (0.00 sec) system.functions \u00b6 Contains information about normal and aggregate functions. mysql> SELECT * FROM system.functions; +-------+ | name | +-------+ | + | | - | | * | | / | | = | | < | | > | | <= | | >= | | != | | <> | | and | | or | | count | | min | | max | | sum | | avg | +-------+ 18 rows in set (0.00 sec) system.contributors \u00b6 Contains information about contributors. The order is random at query execution time. mysql> SELECT * FROM system.contributors LIMIT 20; +-------------------------+ | name | +-------------------------+ | artorias1024 | | BohuTANG | | dependabot[bot] | | dependabot-preview[bot] | | drdr xp | | Eason | | hulunbier | | jyizheng | | leiysky | | smallfish | | sundy-li | | sundyli | | taiyang-li | | TLightSky | | Winter Zhang | | wubx | | yizheng | | Yizheng Jiao | | zhang2014 | | zhihanz | +-------------------------+ 20 rows in set (0.00 sec)","title":"System Tables"},{"location":"system/system-tables/#systemnumbers","text":"This table contains a single UInt64 column named number that contains almost all the natural numbers starting from zero. You can use this table for tests, or if you need to do a brute force search. Reads from this table are parallelized too. Used for tests. mysql> SELECT avg(number) FROM numbers(100000000); +-------------+ | avg(number) | +-------------+ | 49999999.5 | +-------------+ 1 row in set (0.04 sec)","title":"system.numbers"},{"location":"system/system-tables/#systemnumbers_mt","text":"The same as system.numbers","title":"system.numbers_mt"},{"location":"system/system-tables/#systemsettings","text":"Contains information about session settings for current user. mysql> SELECT * FROM system.settings; +----------------+---------+---------------------------------------------------------------------------------------------------+ | name | value | description | +----------------+---------+---------------------------------------------------------------------------------------------------+ | max_block_size | 10000 | Maximum block size for reading | | max_threads | 8 | The maximum number of threads to execute the request. By default, it is determined automatically. | | default_db | default | The default database for current session | +----------------+---------+---------------------------------------------------------------------------------------------------+ 3 rows in set (0.00 sec)","title":"system.settings"},{"location":"system/system-tables/#systemfunctions","text":"Contains information about normal and aggregate functions. mysql> SELECT * FROM system.functions; +-------+ | name | +-------+ | + | | - | | * | | / | | = | | < | | > | | <= | | >= | | != | | <> | | and | | or | | count | | min | | max | | sum | | avg | +-------+ 18 rows in set (0.00 sec)","title":"system.functions"},{"location":"system/system-tables/#systemcontributors","text":"Contains information about contributors. The order is random at query execution time. mysql> SELECT * FROM system.contributors LIMIT 20; +-------------------------+ | name | +-------------------------+ | artorias1024 | | BohuTANG | | dependabot[bot] | | dependabot-preview[bot] | | drdr xp | | Eason | | hulunbier | | jyizheng | | leiysky | | smallfish | | sundy-li | | sundyli | | taiyang-li | | TLightSky | | Winter Zhang | | wubx | | yizheng | | Yizheng Jiao | | zhang2014 | | zhihanz | +-------------------------+ 20 rows in set (0.00 sec)","title":"system.contributors"}]}